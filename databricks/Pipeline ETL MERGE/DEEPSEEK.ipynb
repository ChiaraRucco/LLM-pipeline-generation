{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6013620-52de-4e21-aa7c-ea5c0d9a24a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You are an expert Python developer specializing in the Databricks environment. Your task is to create a complete Python script to be executed within a Databricks notebook. The script must perform the following operations:\n",
    "1.\tData Retrieval from SpaceX API:\n",
    "o\tInteract with the SpaceX v3 REST API (https://api.spacexdata.com/v3).\n",
    "o\tRetrieve data from two specific endpoints: \n",
    "\tAll launches: https://api.spacexdata.com/v3/launches\n",
    "\tAll rockets: https://api.spacexdata.com/v3/rockets\n",
    "o\tHandle potential errors during the API calls (e.g., timeouts, non-200 status codes).\n",
    "2.\tMerge Operation:\n",
    "o\tPerform a \"merge\" (or join) operation between the launches data and the rockets data.\n",
    "o\tMerge Logic: For each launch record, add the rocket's name (rocket_name) from the rockets dataset. The match should be based on the rocket.rocket_id field present in each launch record corresponding to the rocket_id field in each rocket record.\n",
    "o\tThe final result should be a list of dictionaries, where each dictionary represents a launch enriched with its corresponding rocket name.\n",
    "3.\tControl Parameters and Debugging:\n",
    "o\tInclude variables at the beginning of the script to define the API endpoint URLs, making them easily modifiable.\n",
    "o\tUse Python's standard logging module to provide informative output during execution. Configure logging to display messages at the INFO level.\n",
    "o\tLog key messages such as: starting data retrieval, number of launches retrieved, number of rockets retrieved, starting merge operation, number of records after merge, starting upload to httpbin, upload outcome.\n",
    "4.\tExecution Time Measurement:\n",
    "o\tCode Execution Time: Measure the time taken to perform the main operations (data retrieval + merge). Print this time after the merge operation is complete.\n",
    "o\tPipeline Execution Time: Measure the total execution time of the entire script (from the beginning until after the upload to httpbin). Print this total time at the end of the script. Use Python's time module.\n",
    "5.\tUpload Result:\n",
    "o\tTake the resulting list from the merge operation.\n",
    "o\tSerialize it into JSON format.\n",
    "o\tMake an HTTP POST request to the https://httpbin.org/post endpoint, sending the resulting JSON data in the request body.\n",
    "o\tVerify the response from httpbin.org (e.g., check the status code) and log the outcome of the upload operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acadff3a-4609-4a37-a316-cd5989c71be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import logging\n",
    "import time\n",
    "import json\n",
    "from datetime import timedelta\n",
    "\n",
    "# Configuration parameters\n",
    "LAUNCHES_API_URL = \"https://api.spacexdata.com/v3/launches\"\n",
    "ROCKETS_API_URL = \"https://api.spacexdata.com/v3/rockets\"\n",
    "HTTPBIN_UPLOAD_URL = \"https://httpbin.org/post\"\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def fetch_data(url, data_name):\n",
    "    \"\"\"Fetch data from API with error handling\"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Starting {data_name} data retrieval from {url}\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()  # Raises exception for 4XX/5XX status codes\n",
    "        \n",
    "        data = response.json()\n",
    "        elapsed = time.time() - start_time\n",
    "        logger.info(f\"Retrieved {len(data)} {data_name} records in {elapsed:.2f} seconds\")\n",
    "        return data\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Error fetching {data_name} data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def merge_launches_with_rockets(launches, rockets):\n",
    "    \"\"\"Merge launches data with rockets data\"\"\"\n",
    "    logger.info(\"Starting merge operation\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create a lookup dictionary for rockets\n",
    "    rocket_lookup = {rocket['rocket_id']: rocket['rocket_name'] for rocket in rockets}\n",
    "    \n",
    "    # Enrich each launch with rocket name\n",
    "    enriched_launches = []\n",
    "    for launch in launches:\n",
    "        try:\n",
    "            rocket_id = launch['rocket']['rocket_id']\n",
    "            rocket_name = rocket_lookup.get(rocket_id, \"Unknown\")\n",
    "            \n",
    "            # Create a new dictionary to avoid modifying the original\n",
    "            enriched_launch = launch.copy()\n",
    "            enriched_launch['rocket_name'] = rocket_name\n",
    "            enriched_launches.append(enriched_launch)\n",
    "            \n",
    "        except KeyError as e:\n",
    "            logger.warning(f\"Skipping launch due to missing key: {str(e)}\")\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    logger.info(f\"Merged {len(enriched_launches)} records in {elapsed:.2f} seconds\")\n",
    "    return enriched_launches\n",
    "\n",
    "def upload_to_httpbin(data):\n",
    "    \"\"\"Upload data to httpbin and verify response\"\"\"\n",
    "    logger.info(f\"Starting upload to {HTTPBIN_UPLOAD_URL}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        json_data = json.dumps(data)\n",
    "        response = requests.post(HTTPBIN_UPLOAD_URL, data=json_data, headers={'Content-Type': 'application/json'})\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        logger.info(f\"Upload successful (status {response.status_code}) in {elapsed:.2f} seconds\")\n",
    "        return True\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Upload failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution pipeline\"\"\"\n",
    "    pipeline_start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Measure data retrieval and merge time\n",
    "        operations_start_time = time.time()\n",
    "        \n",
    "        # Retrieve data\n",
    "        launches = fetch_data(LAUNCHES_API_URL, \"launches\")\n",
    "        rockets = fetch_data(ROCKETS_API_URL, \"rockets\")\n",
    "        \n",
    "        # Merge data\n",
    "        merged_data = merge_launches_with_rockets(launches, rockets)\n",
    "        \n",
    "        operations_elapsed = time.time() - operations_start_time\n",
    "        logger.info(f\"Data operations completed in {operations_elapsed:.2f} seconds\")\n",
    "        \n",
    "        # Upload result\n",
    "        upload_success = upload_to_httpbin(merged_data)\n",
    "        \n",
    "        # Calculate total pipeline time\n",
    "        pipeline_elapsed = time.time() - pipeline_start_time\n",
    "        logger.info(f\"Total pipeline execution time: {timedelta(seconds=pipeline_elapsed)}\")\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"merged_records\": len(merged_data),\n",
    "            \"upload_success\": upload_success,\n",
    "            \"operations_time_seconds\": operations_elapsed,\n",
    "            \"total_time_seconds\": pipeline_elapsed\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline failed: {str(e)}\")\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "# Execute the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    result = main()\n",
    "    display(result)  # For Databricks notebook display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cd61cbe-c2c1-4973-84e9-6e0224f7c03e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DEEPSEEK",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
