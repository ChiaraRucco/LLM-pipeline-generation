{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1137619-5355-4fef-890f-52fe0a9dd7a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## PROMPT\n",
    "Generate a pipeline for Databricks:\n",
    "* Get Data information from two web addresses:\n",
    "   * Space launches: https://api.spacexdata.com/v3/launches\n",
    "* Aggregate the launches based on launch year\n",
    "* Send the chosen information to the web address: https://httpbin.org/post\n",
    "* The script must provide status updates on its progress, report any errors encountered, confirm the outcome of the final data sending step, and measure/report execution times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60c79a3f-541e-4bbb-baae-0679f03f958a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from time import time\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Initialize logging and timing\n",
    "start_time = time()\n",
    "print(\"Pipeline started at\", start_time)\n",
    "\n",
    "def log_step(message):\n",
    "    \"\"\"Helper function to log steps with timing\"\"\"\n",
    "    elapsed = time() - start_time\n",
    "    print(f\"[{elapsed:.2f}s] {message}\")\n",
    "\n",
    "def handle_error(step_name, e):\n",
    "    \"\"\"Helper function to handle and log errors\"\"\"\n",
    "    error_msg = f\"Error in {step_name}: {str(e)}\"\n",
    "    log_step(error_msg)\n",
    "    raise Exception(error_msg)\n",
    "\n",
    "try:\n",
    "    # Step 1: Fetch data from SpaceX API\n",
    "    log_step(\"Fetching data from SpaceX API...\")\n",
    "    try:\n",
    "        launches_url = \"https://api.spacexdata.com/v3/launches\"\n",
    "        response = requests.get(launches_url)\n",
    "        response.raise_for_status()  # Raises exception for 4XX/5XX errors\n",
    "        launches_data = response.json()\n",
    "        log_step(f\"Successfully fetched {len(launches_data)} launch records\")\n",
    "    except Exception as e:\n",
    "        handle_error(\"fetching SpaceX data\", e)\n",
    "\n",
    "    # Step 2: Create DataFrame from the data\n",
    "    log_step(\"Creating Spark DataFrame...\")\n",
    "    try:\n",
    "        # Define schema for the DataFrame\n",
    "        schema = StructType([\n",
    "            StructField(\"flight_number\", IntegerType(), True),\n",
    "            StructField(\"mission_name\", StringType(), True),\n",
    "            StructField(\"launch_year\", StringType(), True),\n",
    "            StructField(\"launch_success\", StringType(), True),\n",
    "            StructField(\"details\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        # Extract relevant fields and create DataFrame\n",
    "        extracted_data = []\n",
    "        for launch in launches_data:\n",
    "            extracted_data.append((\n",
    "                launch.get(\"flight_number\"),\n",
    "                launch.get(\"mission_name\"),\n",
    "                launch.get(\"launch_year\"),\n",
    "                str(launch.get(\"launch_success\")),\n",
    "                launch.get(\"details\")\n",
    "            ))\n",
    "        \n",
    "        df = spark.createDataFrame(extracted_data, schema)\n",
    "        log_step(f\"DataFrame created with {df.count()} rows\")\n",
    "    except Exception as e:\n",
    "        handle_error(\"creating DataFrame\", e)\n",
    "\n",
    "    # Step 3: Aggregate data by launch year\n",
    "    log_step(\"Aggregating data by launch year...\")\n",
    "    try:\n",
    "        aggregated_df = df.groupBy(\"launch_year\") \\\n",
    "            .agg(\n",
    "                F.count(\"*\").alias(\"launch_count\"),\n",
    "                F.sum(F.when(F.col(\"launch_success\") == \"True\", 1).otherwise(0)).alias(\"successful_launches\"),\n",
    "                F.sum(F.when(F.col(\"launch_success\") == \"False\", 1).otherwise(0)).alias(\"failed_launches\")\n",
    "            ) \\\n",
    "            .orderBy(\"launch_year\")\n",
    "        \n",
    "        log_step(\"Aggregation completed. Sample results:\")\n",
    "        aggregated_df.show(5)\n",
    "    except Exception as e:\n",
    "        handle_error(\"aggregating data\", e)\n",
    "\n",
    "    # Step 4: Prepare data for sending\n",
    "    log_step(\"Preparing data for sending...\")\n",
    "    try:\n",
    "        # Convert to JSON format\n",
    "        aggregated_json = [row.asDict() for row in aggregated_df.collect()]\n",
    "        payload = {\n",
    "            \"source\": \"SpaceX API\",\n",
    "            \"aggregated_launches\": aggregated_json,\n",
    "            \"total_launches\": len(launches_data)\n",
    "        }\n",
    "        log_step(\"Data preparation completed\")\n",
    "    except Exception as e:\n",
    "        handle_error(\"preparing data\", e)\n",
    "\n",
    "    # Step 5: Send data to destination\n",
    "    log_step(\"Sending data to https://httpbin.org/post...\")\n",
    "    try:\n",
    "        destination_url = \"https://httpbin.org/post\"\n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        \n",
    "        send_start = time()\n",
    "        response = requests.post(\n",
    "            destination_url,\n",
    "            data=json.dumps(payload),\n",
    "            headers=headers\n",
    "        )\n",
    "        send_time = time() - send_start\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        \n",
    "        log_step(f\"Data successfully sent in {send_time:.2f}s\")\n",
    "        log_step(f\"Response status: {response.status_code}\")\n",
    "        log_step(\"Response content sample: \" + response.text[:200] + \"...\")\n",
    "        \n",
    "        # Verify the response contains our data\n",
    "        response_data = response.json()\n",
    "        if \"json\" in response_data and response_data[\"json\"].get(\"source\") == \"SpaceX API\":\n",
    "            log_step(\"Destination confirmed receipt of our payload\")\n",
    "        else:\n",
    "            log_step(\"Warning: Destination response format unexpected\")\n",
    "    except Exception as e:\n",
    "        handle_error(\"sending data\", e)\n",
    "\n",
    "    # Final status\n",
    "    total_time = time() - start_time\n",
    "    log_step(f\"Pipeline completed successfully in {total_time:.2f} seconds\")\n",
    "\n",
    "except Exception as e:\n",
    "    log_step(\"Pipeline failed with error: \" + str(e))\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a13fb2f-c1b8-46fb-9a18-996d5e67de7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Aggregate DeepSeek-V3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
