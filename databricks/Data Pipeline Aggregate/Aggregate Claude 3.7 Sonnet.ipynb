{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1137619-5355-4fef-890f-52fe0a9dd7a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## PROMPT\n",
    "Generate a pipeline for Databricks:\n",
    "* Get Data information from two web addresses:\n",
    "   * Space launches: https://api.spacexdata.com/v3/launches\n",
    "* Aggregate the launches based on launch year\n",
    "* Send the chosen information to the web address: https://httpbin.org/post\n",
    "* The script must provide status updates on its progress, report any errors encountered, confirm the outcome of the final data sending step, and measure/report execution times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d94418ec-778f-42d1-a7de-38abc66fc65d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SpaceX Data Pipeline for Databricks\n",
    "# This script fetches SpaceX launch data, aggregates by year, and sends to an endpoint\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, count\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"SpaceX Data Pipeline\").getOrCreate()\n",
    "\n",
    "def log_message(message):\n",
    "    \"\"\"Log a message with timestamp.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"[{timestamp}] {message}\")\n",
    "\n",
    "def fetch_data(url):\n",
    "    \"\"\"Fetch data from the given URL.\"\"\"\n",
    "    log_message(f\"Fetching data from {url}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()  # Raise exception for HTTP errors\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        log_message(f\"Successfully fetched data in {execution_time:.2f} seconds\")\n",
    "        \n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        execution_time = time.time() - start_time\n",
    "        log_message(f\"ERROR: Failed to fetch data from {url} after {execution_time:.2f} seconds\")\n",
    "        log_message(f\"Exception details: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def process_spacex_data(data):\n",
    "    \"\"\"Process SpaceX launch data.\"\"\"\n",
    "    log_message(\"Processing SpaceX launch data\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Convert JSON to DataFrame\n",
    "        df = spark.read.json(spark.sparkContext.parallelize([json.dumps(item) for item in data]))\n",
    "        \n",
    "        # Extract launch year from launch_date_utc\n",
    "        df = df.withColumn(\"launch_year\", year(col(\"launch_date_utc\")))\n",
    "        \n",
    "        # Aggregate launches by year\n",
    "        year_counts = df.groupBy(\"launch_year\").agg(count(\"*\").alias(\"launch_count\"))\n",
    "        \n",
    "        # Convert to dictionary format for easier JSON serialization\n",
    "        result = [{\n",
    "            \"year\": row[\"launch_year\"],\n",
    "            \"launch_count\": row[\"launch_count\"]\n",
    "        } for row in year_counts.collect()]\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        log_message(f\"Successfully processed data in {execution_time:.2f} seconds\")\n",
    "        log_message(f\"Found {len(result)} launch years\")\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        execution_time = time.time() - start_time\n",
    "        log_message(f\"ERROR: Failed to process data after {execution_time:.2f} seconds\")\n",
    "        log_message(f\"Exception details: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def send_data(url, data):\n",
    "    \"\"\"Send data to the specified endpoint.\"\"\"\n",
    "    log_message(f\"Sending data to {url}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        response = requests.post(url, json=data, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        log_message(f\"Successfully sent data in {execution_time:.2f} seconds\")\n",
    "        log_message(f\"Response status code: {response.status_code}\")\n",
    "        \n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        execution_time = time.time() - start_time\n",
    "        log_message(f\"ERROR: Failed to send data to {url} after {execution_time:.2f} seconds\")\n",
    "        log_message(f\"Exception details: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the pipeline.\"\"\"\n",
    "    pipeline_start_time = time.time()\n",
    "    log_message(\"Starting SpaceX data pipeline\")\n",
    "    \n",
    "    try:\n",
    "        # Fetch SpaceX launch data\n",
    "        spacex_url = \"https://api.spacexdata.com/v3/launches\"\n",
    "        spacex_data = fetch_data(spacex_url)\n",
    "        \n",
    "        # Process data\n",
    "        processed_data = process_spacex_data(spacex_data)\n",
    "        \n",
    "        # Send aggregated data to destination\n",
    "        destination_url = \"https://httpbin.org/post\"\n",
    "        response = send_data(destination_url, processed_data)\n",
    "        \n",
    "        # Log final result\n",
    "        pipeline_execution_time = time.time() - pipeline_start_time\n",
    "        log_message(f\"Pipeline completed successfully in {pipeline_execution_time:.2f} seconds\")\n",
    "        log_message(f\"Final response from destination: {json.dumps(response)[:100]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        pipeline_execution_time = time.time() - pipeline_start_time\n",
    "        log_message(f\"ERROR: Pipeline failed after {pipeline_execution_time:.2f} seconds\")\n",
    "        log_message(f\"Exception details: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6ac7495-869a-4601-8754-53854c1bac5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "ERROR 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "229e34fd-73ac-4b6b-a34e-9011d15ab7f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SpaceX Data Pipeline for Databricks\n",
    "# This script fetches SpaceX launch data, aggregates by year, and sends to an endpoint\n",
    "# Compatible with Databricks Connect and serverless compute\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, count\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Spark Session (without needing sparkContext)\n",
    "spark = SparkSession.builder.appName(\"SpaceX Data Pipeline\").getOrCreate()\n",
    "\n",
    "def log_message(message):\n",
    "    \"\"\"Log a message with timestamp.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"[{timestamp}] {message}\")\n",
    "\n",
    "def fetch_data(url):\n",
    "    \"\"\"Fetch data from the given URL.\"\"\"\n",
    "    log_message(f\"Fetching data from {url}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()  # Raise exception for HTTP errors\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        log_message(f\"Successfully fetched data in {execution_time:.2f} seconds\")\n",
    "        \n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        execution_time = time.time() - start_time\n",
    "        log_message(f\"ERROR: Failed to fetch data from {url} after {execution_time:.2f} seconds\")\n",
    "        log_message(f\"Exception details: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def process_spacex_data(data):\n",
    "    \"\"\"Process SpaceX launch data.\"\"\"\n",
    "    log_message(\"Processing SpaceX launch data\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # First convert JSON to pandas DataFrame\n",
    "        pdf = pd.DataFrame(data)\n",
    "        \n",
    "        # Convert pandas DataFrame to Spark DataFrame\n",
    "        df = spark.createDataFrame(pdf)\n",
    "        \n",
    "        # Extract launch year from launch_date_utc\n",
    "        df = df.withColumn(\"launch_year\", year(col(\"launch_date_utc\")))\n",
    "        \n",
    "        # Aggregate launches by year\n",
    "        year_counts = df.groupBy(\"launch_year\").agg(count(\"*\").alias(\"launch_count\"))\n",
    "        \n",
    "        # Convert to pandas DataFrame for easier processing\n",
    "        result_df = year_counts.toPandas()\n",
    "        \n",
    "        # Convert to dictionary format for easier JSON serialization\n",
    "        result = result_df.to_dict(orient=\"records\")\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        log_message(f\"Successfully processed data in {execution_time:.2f} seconds\")\n",
    "        log_message(f\"Found {len(result)} launch years\")\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        execution_time = time.time() - start_time\n",
    "        log_message(f\"ERROR: Failed to process data after {execution_time:.2f} seconds\")\n",
    "        log_message(f\"Exception details: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def send_data(url, data):\n",
    "    \"\"\"Send data to the specified endpoint.\"\"\"\n",
    "    log_message(f\"Sending data to {url}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        response = requests.post(url, json=data, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        execution_time = time.time() - start_time\n",
    "        log_message(f\"Successfully sent data in {execution_time:.2f} seconds\")\n",
    "        log_message(f\"Response status code: {response.status_code}\")\n",
    "        \n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        execution_time = time.time() - start_time\n",
    "        log_message(f\"ERROR: Failed to send data to {url} after {execution_time:.2f} seconds\")\n",
    "        log_message(f\"Exception details: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the pipeline.\"\"\"\n",
    "    pipeline_start_time = time.time()\n",
    "    log_message(\"Starting SpaceX data pipeline\")\n",
    "    \n",
    "    try:\n",
    "        # Fetch SpaceX launch data\n",
    "        spacex_url = \"https://api.spacexdata.com/v3/launches\"\n",
    "        spacex_data = fetch_data(spacex_url)\n",
    "        \n",
    "        # Process data\n",
    "        processed_data = process_spacex_data(spacex_data)\n",
    "        \n",
    "        # Send aggregated data to destination\n",
    "        destination_url = \"https://httpbin.org/post\"\n",
    "        response = send_data(destination_url, processed_data)\n",
    "        \n",
    "        # Log final result\n",
    "        pipeline_execution_time = time.time() - pipeline_start_time\n",
    "        log_message(f\"Pipeline completed successfully in {pipeline_execution_time:.2f} seconds\")\n",
    "        log_message(f\"Final response from destination: {json.dumps(response)[:100]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        pipeline_execution_time = time.time() - pipeline_start_time\n",
    "        log_message(f\"ERROR: Pipeline failed after {pipeline_execution_time:.2f} seconds\")\n",
    "        log_message(f\"Exception details: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "105efcb5-576a-4264-b13c-ca600449d40c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Aggregate Claude 3.7 Sonnet",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
