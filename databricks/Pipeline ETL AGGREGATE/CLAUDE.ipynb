{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6013620-52de-4e21-aa7c-ea5c0d9a24a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You are an expert Python developer specializing in the Databricks environment. Your task is to create a complete Python script to be executed within a Databricks notebook. The script must perform the following operations:\n",
    "1.\tData Retrieval from SpaceX API:\n",
    "o\tInteract with the SpaceX v3 REST API (https://api.spacexdata.com/v3).\n",
    "o\tRetrieve data from one specific endpoint: \n",
    "\tAll launches: https://api.spacexdata.com/v3/launches\n",
    "o\tHandle potential errors during the API calls (e.g., timeouts, non-200 status codes).\n",
    "2.\tAggregate Operation:\n",
    "o\tPerform a simple \"aggregate\" operation on the retrieved launch data.\n",
    "o\tAggregation Logic: Calculate the total number of launches for each year.\n",
    "o\tReturn a print with the number of aggregate launches for each year\n",
    "3.\tControl Parameters and Debugging:\n",
    "o\tInclude a variable at the beginning of the script to define the API endpoint URL, making it easily modifiable: \n",
    "\tAPI_ENDPOINT_URL = \"https://api.spacexdata.com/v3/launches\"\n",
    "o\tUse Python's standard logging module to provide informative output during execution. Configure logging to display messages at the INFO level.\n",
    "o\tLog key messages such as: starting data retrieval, number of launches retrieved, starting aggregation, aggregation complete, number of years aggregated, starting upload to httpbin, upload outcome.\n",
    "4.\tExecution Time Measurement:\n",
    "o\tCode Execution Time: Measure the time taken to perform the main operations (data retrieval + aggregation). Print this time after the aggregation operation is complete.\n",
    "o\tPipeline Execution Time: Measure the total execution time of the entire script (from the beginning until after the upload to httpbin). Print this total time at the end of the script. Use Python's time module.\n",
    "5.\tUpload Result:\n",
    "o\tTake the resulting aggregated summary dictionary (year counts) from the aggregate operation.\n",
    "o\tSerialize it into JSON format.\n",
    "o\tMake an HTTP POST request to the https://httpbin.org/post endpoint, sending the resulting aggregated JSON data in the request body.\n",
    "o\tVerify the response from httpbin.org (e.g., check the status code) and log the outcome of the upload operation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acadff3a-4609-4a37-a316-cd5989c71be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Control Parameters\n",
    "API_ENDPOINT_URL = \"https://api.spacexdata.com/v3/launches\"\n",
    "HTTPBIN_URL = \"https://httpbin.org/post\"\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def retrieve_spacex_data(api_url: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Retrieve data from SpaceX API with error handling\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting data retrieval from {api_url}\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(api_url, timeout=30)\n",
    "        response.raise_for_status()  # Raise exception for non-200 status codes\n",
    "        data = response.json()\n",
    "        logger.info(f\"Successfully retrieved {len(data)} launches\")\n",
    "        return data\n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.error(\"Request timed out\")\n",
    "        raise\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        logger.error(f\"HTTP error occurred: {e}\")\n",
    "        raise\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Error retrieving data: {e}\")\n",
    "        raise\n",
    "    except json.JSONDecodeError:\n",
    "        logger.error(\"Error decoding JSON response\")\n",
    "        raise\n",
    "\n",
    "def aggregate_launches_by_year(launches_data: List[Dict[str, Any]]) -> Dict[int, int]:\n",
    "    \"\"\"\n",
    "    Aggregate launches by year\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting aggregation of launches by year\")\n",
    "    \n",
    "    yearly_launches = {}\n",
    "    \n",
    "    for launch in launches_data:\n",
    "        if launch.get('launch_date_utc'):\n",
    "            # Parse the UTC launch date\n",
    "            launch_date = datetime.strptime(launch['launch_date_utc'], \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "            year = launch_date.year\n",
    "            \n",
    "            # Increment count for this year\n",
    "            if year in yearly_launches:\n",
    "                yearly_launches[year] += 1\n",
    "            else:\n",
    "                yearly_launches[year] = 1\n",
    "    \n",
    "    # Sort by year for better presentation\n",
    "    sorted_yearly_launches = {year: yearly_launches[year] for year in sorted(yearly_launches.keys())}\n",
    "    \n",
    "    logger.info(f\"Aggregation complete. Found launches in {len(sorted_yearly_launches)} different years\")\n",
    "    \n",
    "    # Print the results as required\n",
    "    for year, count in sorted_yearly_launches.items():\n",
    "        print(f\"Year {year}: {count} launches\")\n",
    "    \n",
    "    return sorted_yearly_launches\n",
    "\n",
    "def upload_to_httpbin(data: Dict[int, int]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Upload aggregated data to httpbin.org\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting upload to {HTTPBIN_URL}\")\n",
    "    \n",
    "    # Convert dictionary with integer keys to string keys for JSON serialization\n",
    "    json_data = {str(year): count for year, count in data.items()}\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(HTTPBIN_URL, json=json_data, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        logger.info(\"Upload successful\")\n",
    "        return result\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Error uploading data: {e}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    # Start measuring total execution time\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Start measuring operations execution time\n",
    "        operations_start_time = time.time()\n",
    "        \n",
    "        # Retrieve data\n",
    "        launches_data = retrieve_spacex_data(API_ENDPOINT_URL)\n",
    "        \n",
    "        # Perform aggregation\n",
    "        yearly_launches = aggregate_launches_by_year(launches_data)\n",
    "        \n",
    "        # Calculate operations execution time\n",
    "        operations_end_time = time.time()\n",
    "        operations_execution_time = operations_end_time - operations_start_time\n",
    "        logger.info(f\"Data retrieval and aggregation completed in {operations_execution_time:.2f} seconds\")\n",
    "        print(f\"Operations execution time (data retrieval + aggregation): {operations_execution_time:.2f} seconds\")\n",
    "        \n",
    "        # Upload data to httpbin\n",
    "        response = upload_to_httpbin(yearly_launches)\n",
    "        logger.info(f\"Upload response status: {response.get('status', 'unknown')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Script execution failed: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        # Calculate total execution time\n",
    "        total_end_time = time.time()\n",
    "        total_execution_time = total_end_time - total_start_time\n",
    "        logger.info(f\"Script completed in {total_execution_time:.2f} seconds\")\n",
    "        print(f\"Total pipeline execution time: {total_execution_time:.2f} seconds\")\n",
    "\n",
    "# Execute the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6c2d32e-121d-424b-ab13-45e41c13e8b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CLAUDE",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
