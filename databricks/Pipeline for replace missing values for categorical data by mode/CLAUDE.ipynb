{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6013620-52de-4e21-aa7c-ea5c0d9a24a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You are an expert Python developer specializing in the Databricks environment. Your task is to create a complete Python script to be executed within a Databricks notebook. The script must perform the following operations:\n",
    "1.\tData Retrieval from SpaceX API:\n",
    "o\tInteract with the SpaceX v3 REST API (https://api.spacexdata.com/v3).\n",
    "o\tRetrieve data from one specific endpoint likely containing categorical data where missing values might occur: \n",
    "\tAll Cores: https://api.spacexdata.com/v3/cores (Fields like status, block could be candidates)\n",
    "\tAlternative: All Launches: https://api.spacexdata.com/v3/launches (Fields like launch_site.site_name, rocket.rocket_name)\n",
    "o\tHandle potential errors during the API calls (e.g., timeouts, non-200 status codes).\n",
    "2.\tMissing Value Imputation (Mode):\n",
    "o\tPerform mode imputation on the retrieved data (list of dictionaries).\n",
    "o\tImputation Logic: \n",
    "\tIdentify Categorical Fields: First, automatically identify the keys/fields within the dictionaries that predominantly contain categorical data (e.g., strings - str). You might need to inspect the first few records or a sample, or iterate through checking types.\n",
    "\tCalculate Mode per Field: For each identified categorical field, determine the mode (the most frequent value) using only the existing, non-missing (not None) values across all records in the dataset. The collections.Counter class is suitable for this.\n",
    "\tHandle Ties: If multiple values share the highest frequency (a tie for the mode), select any one of them as the mode (e.g., the one that appears first alphabetically or the first one encountered during counting).\n",
    "\tImpute Missing Values: Iterate through the dataset again. For each categorical field, replace any missing values (represented as None) with the pre-calculated mode for that specific field.\n",
    "\tHandle Edge Cases: If a categorical field contains only missing values (or no non-missing values to calculate a mode), log a warning and leave the missing values as None.\n",
    "o\tThe final result should be the original list of dictionaries, but with missing categorical values replaced by the calculated mode for their respective fields.\n",
    "3.\tControl Parameters and Debugging:\n",
    "o\tInclude a variable at the beginning of the script to define the API endpoint URL, making it easily modifiable: \n",
    "\tAPI_ENDPOINT_URL = \"https://api.spacexdata.com/v3/cores\" #(or /launches)\n",
    "o\tUse Python's standard logging module to provide informative output during execution. Configure logging to display messages at the INFO level.\n",
    "o\tLog key messages such as: starting data retrieval, number of records retrieved, starting mode imputation process, identified categorical fields potentially needing imputation (e.g., ['status', 'block', ...]), calculated mode for field X, number of missing values imputed for field X, any warnings for fields with no calculable mode, mode imputation complete, starting upload to httpbin, upload outcome.\n",
    "4.\tExecution Time Measurement:\n",
    "o\tCode Execution Time: Measure the time taken to perform the main operations (data retrieval + mode imputation). Print this time after the imputation operation is complete.\n",
    "o\tPipeline Execution Time: Measure the total execution time of the entire script (from the beginning until after the upload to httpbin). Print this total time at the end of the script. Use Python's time module.\n",
    "5.\tUpload Result:\n",
    "o\tTake the resulting imputed list of dictionaries from the mode imputation operation.\n",
    "o\tSerialize it into JSON format.\n",
    "o\tMake an HTTP POST request to the https://httpbin.org/post endpoint, sending the resulting imputed JSON data in the request body.\n",
    "o\tVerify the response from httpbin.org (e.g., check the status code) and log the outcome of the upload operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acadff3a-4609-4a37-a316-cd5989c71be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# SpaceX API Data Processing with Mode Imputation\n",
    "# For Databricks Environment\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from collections import Counter\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# API endpoint configuration\n",
    "API_ENDPOINT_URL = \"https://api.spacexdata.com/v3/cores\"  # Alternative: \"https://api.spacexdata.com/v3/launches\"\n",
    "HTTPBIN_URL = \"https://httpbin.org/post\"\n",
    "\n",
    "def retrieve_spacex_data(url: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Retrieves data from the SpaceX API and handles potential errors.\n",
    "    \n",
    "    Args:\n",
    "        url: The API endpoint URL\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing the API response data\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting data retrieval from {url}\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, timeout=30)\n",
    "        response.raise_for_status()  # Raise exception for 4XX/5XX responses\n",
    "        data = response.json()\n",
    "        logger.info(f\"Successfully retrieved {len(data)} records from the API\")\n",
    "        return data\n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.error(\"Request timed out. Check network connection or API availability.\")\n",
    "        raise\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        logger.error(f\"HTTP error occurred: {e}\")\n",
    "        raise\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Error during API request: {e}\")\n",
    "        raise\n",
    "    except json.JSONDecodeError:\n",
    "        logger.error(\"Failed to parse API response as JSON\")\n",
    "        raise\n",
    "\n",
    "def identify_categorical_fields(data: List[Dict[str, Any]], sample_size: int = 10) -> List[str]:\n",
    "    \"\"\"\n",
    "    Identifies potentially categorical fields in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        data: List of dictionaries\n",
    "        sample_size: Number of records to examine\n",
    "        \n",
    "    Returns:\n",
    "        List of field names that appear to be categorical\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        return []\n",
    "    \n",
    "    # Sample data to examine\n",
    "    sample = data[:min(sample_size, len(data))]\n",
    "    \n",
    "    # Track fields and their value types\n",
    "    field_types = {}\n",
    "    categorical_fields = []\n",
    "    \n",
    "    # Examine each record in the sample\n",
    "    for record in sample:\n",
    "        for field, value in record.items():\n",
    "            # Skip None values\n",
    "            if value is None:\n",
    "                continue\n",
    "                \n",
    "            # For nested dictionaries, skip processing\n",
    "            if isinstance(value, dict):\n",
    "                continue\n",
    "                \n",
    "            # For lists, skip processing\n",
    "            if isinstance(value, list):\n",
    "                continue\n",
    "                \n",
    "            # Update field_types dictionary\n",
    "            if field not in field_types:\n",
    "                field_types[field] = []\n",
    "            \n",
    "            field_types[field].append(type(value))\n",
    "    \n",
    "    # Identify fields that predominantly contain strings (categorical)\n",
    "    for field, types in field_types.items():\n",
    "        # Calculate percentage of string values\n",
    "        if types:\n",
    "            string_percentage = types.count(str) / len(types)\n",
    "            \n",
    "            # If more than 70% of non-None values are strings, consider it categorical\n",
    "            if string_percentage > 0.7:\n",
    "                categorical_fields.append(field)\n",
    "    \n",
    "    return categorical_fields\n",
    "\n",
    "def calculate_modes(data: List[Dict[str, Any]], categorical_fields: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calculates the mode for each categorical field.\n",
    "    \n",
    "    Args:\n",
    "        data: List of dictionaries\n",
    "        categorical_fields: List of field names considered categorical\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping field names to their mode values\n",
    "    \"\"\"\n",
    "    field_modes = {}\n",
    "    \n",
    "    for field in categorical_fields:\n",
    "        # Collect all non-None values for this field\n",
    "        field_values = [record[field] for record in data if field in record and record[field] is not None]\n",
    "        \n",
    "        if field_values:\n",
    "            # Calculate mode using Counter\n",
    "            counter = Counter(field_values)\n",
    "            mode_value = counter.most_common(1)[0][0]\n",
    "            field_modes[field] = mode_value\n",
    "            logger.info(f\"Field '{field}': Mode calculated as '{mode_value}'\")\n",
    "        else:\n",
    "            logger.warning(f\"Field '{field}': Contains only missing values, no mode calculated\")\n",
    "            field_modes[field] = None\n",
    "            \n",
    "    return field_modes\n",
    "\n",
    "def perform_mode_imputation(data: List[Dict[str, Any]], field_modes: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Imputes missing values with the mode for each categorical field.\n",
    "    \n",
    "    Args:\n",
    "        data: List of dictionaries\n",
    "        field_modes: Dictionary mapping field names to their mode values\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries with missing values imputed\n",
    "    \"\"\"\n",
    "    imputed_data = data.copy()\n",
    "    imputation_counts = {field: 0 for field in field_modes}\n",
    "    \n",
    "    for i, record in enumerate(imputed_data):\n",
    "        for field, mode_value in field_modes.items():\n",
    "            # Skip fields with no calculable mode\n",
    "            if mode_value is None:\n",
    "                continue\n",
    "                \n",
    "            # Check if field exists and is None\n",
    "            if field in record and record[field] is None:\n",
    "                record[field] = mode_value\n",
    "                imputation_counts[field] += 1\n",
    "    \n",
    "    # Log imputation counts\n",
    "    for field, count in imputation_counts.items():\n",
    "        logger.info(f\"Field '{field}': Imputed {count} missing values\")\n",
    "    \n",
    "    return imputed_data\n",
    "\n",
    "def upload_to_httpbin(data: List[Dict[str, Any]]) -> requests.Response:\n",
    "    \"\"\"\n",
    "    Uploads the processed data to httpbin.org.\n",
    "    \n",
    "    Args:\n",
    "        data: List of dictionaries to upload\n",
    "        \n",
    "    Returns:\n",
    "        Response object from the HTTP request\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting upload to httpbin.org\")\n",
    "    \n",
    "    try:\n",
    "        json_data = json.dumps(data)\n",
    "        response = requests.post(HTTPBIN_URL, data=json_data, headers={'Content-Type': 'application/json'}, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        logger.info(f\"Upload successful: Status code {response.status_code}\")\n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Error during upload: {e}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the data processing pipeline.\"\"\"\n",
    "    # Start timing for the entire pipeline\n",
    "    pipeline_start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Data retrieval\n",
    "        raw_data = retrieve_spacex_data(API_ENDPOINT_URL)\n",
    "        \n",
    "        # Start timing for data processing\n",
    "        processing_start_time = time.time()\n",
    "        \n",
    "        # Identify categorical fields\n",
    "        logger.info(\"Identifying categorical fields for imputation\")\n",
    "        categorical_fields = identify_categorical_fields(raw_data)\n",
    "        logger.info(f\"Identified categorical fields: {categorical_fields}\")\n",
    "        \n",
    "        # Calculate mode for each categorical field\n",
    "        logger.info(\"Starting mode calculation\")\n",
    "        field_modes = calculate_modes(raw_data, categorical_fields)\n",
    "        \n",
    "        # Perform mode imputation\n",
    "        logger.info(\"Starting mode imputation process\")\n",
    "        imputed_data = perform_mode_imputation(raw_data, field_modes)\n",
    "        \n",
    "        # End timing for data processing\n",
    "        processing_end_time = time.time()\n",
    "        processing_time = processing_end_time - processing_start_time\n",
    "        logger.info(f\"Data processing (imputation) completed in {processing_time:.2f} seconds\")\n",
    "        \n",
    "        # Upload imputed data to httpbin\n",
    "        response = upload_to_httpbin(imputed_data)\n",
    "        \n",
    "        # End timing for the entire pipeline\n",
    "        pipeline_end_time = time.time()\n",
    "        pipeline_time = pipeline_end_time - pipeline_start_time\n",
    "        logger.info(f\"Total pipeline execution completed in {pipeline_time:.2f} seconds\")\n",
    "        \n",
    "        # Return imputed data for potential further use in the notebook\n",
    "        return imputed_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline execution failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Execute the pipeline when the notebook cell is run\n",
    "if __name__ == \"__main__\":\n",
    "    result = main()\n",
    "    \n",
    "    # Display some results in the notebook for verification\n",
    "    print(f\"Processed {len(result)} SpaceX records with mode imputation\")\n",
    "    \n",
    "    # In a Databricks notebook, you can also display the first few records\n",
    "    # Display the first 5 records\n",
    "    print(\"\\nSample of processed data (first 5 records):\")\n",
    "    for i, record in enumerate(result[:5]):\n",
    "        print(f\"Record {i+1}: {record}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6c2d32e-121d-424b-ab13-45e41c13e8b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CLAUDE",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
