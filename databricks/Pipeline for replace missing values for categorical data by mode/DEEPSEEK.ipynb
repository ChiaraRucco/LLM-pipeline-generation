{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6013620-52de-4e21-aa7c-ea5c0d9a24a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You are an expert Python developer specializing in the Databricks environment. Your task is to create a complete Python script to be executed within a Databricks notebook. The script must perform the following operations:\n",
    "1.\tData Retrieval from SpaceX API:\n",
    "o\tInteract with the SpaceX v3 REST API (https://api.spacexdata.com/v3).\n",
    "o\tRetrieve data from one specific endpoint likely containing categorical data where missing values might occur: \n",
    "\tAll Cores: https://api.spacexdata.com/v3/cores (Fields like status, block could be candidates)\n",
    "\tAlternative: All Launches: https://api.spacexdata.com/v3/launches (Fields like launch_site.site_name, rocket.rocket_name)\n",
    "o\tHandle potential errors during the API calls (e.g., timeouts, non-200 status codes).\n",
    "2.\tMissing Value Imputation (Mode):\n",
    "o\tPerform mode imputation on the retrieved data (list of dictionaries).\n",
    "o\tImputation Logic: \n",
    "\tIdentify Categorical Fields: First, automatically identify the keys/fields within the dictionaries that predominantly contain categorical data (e.g., strings - str). You might need to inspect the first few records or a sample, or iterate through checking types.\n",
    "\tCalculate Mode per Field: For each identified categorical field, determine the mode (the most frequent value) using only the existing, non-missing (not None) values across all records in the dataset. The collections.Counter class is suitable for this.\n",
    "\tHandle Ties: If multiple values share the highest frequency (a tie for the mode), select any one of them as the mode (e.g., the one that appears first alphabetically or the first one encountered during counting).\n",
    "\tImpute Missing Values: Iterate through the dataset again. For each categorical field, replace any missing values (represented as None) with the pre-calculated mode for that specific field.\n",
    "\tHandle Edge Cases: If a categorical field contains only missing values (or no non-missing values to calculate a mode), log a warning and leave the missing values as None.\n",
    "o\tThe final result should be the original list of dictionaries, but with missing categorical values replaced by the calculated mode for their respective fields.\n",
    "3.\tControl Parameters and Debugging:\n",
    "o\tInclude a variable at the beginning of the script to define the API endpoint URL, making it easily modifiable: \n",
    "\tAPI_ENDPOINT_URL = \"https://api.spacexdata.com/v3/cores\" #(or /launches)\n",
    "o\tUse Python's standard logging module to provide informative output during execution. Configure logging to display messages at the INFO level.\n",
    "o\tLog key messages such as: starting data retrieval, number of records retrieved, starting mode imputation process, identified categorical fields potentially needing imputation (e.g., ['status', 'block', ...]), calculated mode for field X, number of missing values imputed for field X, any warnings for fields with no calculable mode, mode imputation complete, starting upload to httpbin, upload outcome.\n",
    "4.\tExecution Time Measurement:\n",
    "o\tCode Execution Time: Measure the time taken to perform the main operations (data retrieval + mode imputation). Print this time after the imputation operation is complete.\n",
    "o\tPipeline Execution Time: Measure the total execution time of the entire script (from the beginning until after the upload to httpbin). Print this total time at the end of the script. Use Python's time module.\n",
    "5.\tUpload Result:\n",
    "o\tTake the resulting imputed list of dictionaries from the mode imputation operation.\n",
    "o\tSerialize it into JSON format.\n",
    "o\tMake an HTTP POST request to the https://httpbin.org/post endpoint, sending the resulting imputed JSON data in the request body.\n",
    "o\tVerify the response from httpbin.org (e.g., check the status code) and log the outcome of the upload operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acadff3a-4609-4a37-a316-cd5989c71be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import logging\n",
    "import time\n",
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Control parameters\n",
    "API_ENDPOINT_URL = \"https://api.spacexdata.com/v3/cores\"  # Can be changed to /launches\n",
    "HTTPBIN_UPLOAD_URL = \"https://httpbin.org/post\"\n",
    "\n",
    "def retrieve_spacex_data(url):\n",
    "    \"\"\"\n",
    "    Retrieve data from SpaceX API with error handling\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting data retrieval from {url}\")\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()  # Raises exception for 4XX/5XX status codes\n",
    "        data = response.json()\n",
    "        logger.info(f\"Successfully retrieved {len(data)} records\")\n",
    "        return data\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Failed to retrieve data from API: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def identify_categorical_fields(data, sample_size=50):\n",
    "    \"\"\"\n",
    "    Identify fields that predominantly contain categorical data (strings)\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        return []\n",
    "    \n",
    "    # Use a sample of the data to determine field types\n",
    "    sample = data[:min(sample_size, len(data))]\n",
    "    field_types = {}\n",
    "    \n",
    "    for record in sample:\n",
    "        for field, value in record.items():\n",
    "            if field not in field_types:\n",
    "                field_types[field] = []\n",
    "            field_types[field].append(type(value).__name__)\n",
    "    \n",
    "    categorical_fields = []\n",
    "    for field, types in field_types.items():\n",
    "        # Consider a field categorical if >50% of values are strings\n",
    "        str_count = types.count('str')\n",
    "        if str_count / len(types) > 0.5:\n",
    "            categorical_fields.append(field)\n",
    "    \n",
    "    logger.info(f\"Identified potential categorical fields: {categorical_fields}\")\n",
    "    return categorical_fields\n",
    "\n",
    "def calculate_modes(data, categorical_fields):\n",
    "    \"\"\"\n",
    "    Calculate the mode for each categorical field\n",
    "    \"\"\"\n",
    "    modes = {}\n",
    "    \n",
    "    for field in categorical_fields:\n",
    "        # Collect all non-None values for the field\n",
    "        values = [record.get(field) for record in data if record.get(field) is not None]\n",
    "        \n",
    "        if not values:\n",
    "            logger.warning(f\"Field '{field}' contains no non-missing values to calculate mode\")\n",
    "            modes[field] = None\n",
    "            continue\n",
    "        \n",
    "        # Count frequency of each value\n",
    "        counter = Counter(values)\n",
    "        max_count = max(counter.values())\n",
    "        candidates = [val for val, count in counter.items() if count == max_count]\n",
    "        \n",
    "        # Handle ties by selecting the first candidate (could be changed to alphabetical)\n",
    "        mode = candidates[0]\n",
    "        modes[field] = mode\n",
    "        logger.info(f\"Calculated mode for field '{field}': '{mode}' (appears {max_count} times)\")\n",
    "    \n",
    "    return modes\n",
    "\n",
    "def perform_mode_imputation(data, categorical_fields, modes):\n",
    "    \"\"\"\n",
    "    Perform mode imputation on the data for specified fields\n",
    "    \"\"\"\n",
    "    imputed_data = []\n",
    "    imputation_counts = {field: 0 for field in categorical_fields}\n",
    "    \n",
    "    for record in data:\n",
    "        new_record = record.copy()\n",
    "        for field in categorical_fields:\n",
    "            if field in new_record and new_record[field] is None and modes[field] is not None:\n",
    "                new_record[field] = modes[field]\n",
    "                imputation_counts[field] += 1\n",
    "        imputed_data.append(new_record)\n",
    "    \n",
    "    for field, count in imputation_counts.items():\n",
    "        if count > 0:\n",
    "            logger.info(f\"Imputed {count} missing values for field '{field}'\")\n",
    "    \n",
    "    return imputed_data\n",
    "\n",
    "def upload_to_httpbin(data, url):\n",
    "    \"\"\"\n",
    "    Upload the processed data to httpbin\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting upload to {url}\")\n",
    "    try:\n",
    "        json_data = json.dumps(data)\n",
    "        response = requests.post(url, data=json_data, headers={'Content-Type': 'application/json'}, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        logger.info(f\"Upload successful. Response status: {response.status_code}\")\n",
    "        return True\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Upload failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    # Start total pipeline timer\n",
    "    pipeline_start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Start data retrieval and imputation timer\n",
    "        processing_start_time = time.time()\n",
    "        \n",
    "        # 1. Retrieve data from SpaceX API\n",
    "        data = retrieve_spacex_data(API_ENDPOINT_URL)\n",
    "        \n",
    "        if not data:\n",
    "            logger.error(\"No data retrieved from API. Exiting.\")\n",
    "            return\n",
    "        \n",
    "        # 2. Perform mode imputation\n",
    "        categorical_fields = identify_categorical_fields(data)\n",
    "        \n",
    "        if not categorical_fields:\n",
    "            logger.info(\"No categorical fields identified. No imputation needed.\")\n",
    "            imputed_data = data\n",
    "        else:\n",
    "            modes = calculate_modes(data, categorical_fields)\n",
    "            imputed_data = perform_mode_imputation(data, categorical_fields, modes)\n",
    "        \n",
    "        # Log processing time\n",
    "        processing_time = time.time() - processing_start_time\n",
    "        logger.info(f\"Data processing completed in {processing_time:.2f} seconds\")\n",
    "        \n",
    "        # 3. Upload result to httpbin\n",
    "        upload_result = upload_to_httpbin(imputed_data, HTTPBIN_UPLOAD_URL)\n",
    "        \n",
    "        # Log total pipeline time\n",
    "        total_time = time.time() - pipeline_start_time\n",
    "        logger.info(f\"Total pipeline execution time: {total_time:.2f} seconds\")\n",
    "        \n",
    "        return imputed_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during processing: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8e20233-f174-4ba7-8391-c7cee400d871",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acefd7a9-5714-4ea5-b7e0-ce659c520a3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72cdddb2-10fe-4846-a456-65c4e1378342",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DEEPSEEK",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
