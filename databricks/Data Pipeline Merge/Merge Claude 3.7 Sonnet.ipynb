{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1137619-5355-4fef-890f-52fe0a9dd7a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## PROMPT\n",
    "Generate a pipeline for Databricks:\n",
    "\n",
    "Get Data information from two web addresses:\n",
    "\n",
    "Space launches: https://api.spacexdata.com/v3/launches\n",
    "\n",
    "Space rockets: https://api.spacexdata.com/v3/rockets\n",
    "\n",
    "Combine each launch record with the name of the corresponding rocket.\n",
    "\n",
    "Send the final result (launches with rocket names) to the web address: https://httpbin.org/post\n",
    "\n",
    "The script must provide status updates on its progress, report any errors encountered, confirm the outcome of the final data sending step, and measure/report execution times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d94418ec-778f-42d1-a7de-38abc66fc65d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks SpaceX Data Processing Pipeline\n",
    "# This pipeline fetches SpaceX launch and rocket data, joins them, and posts results to an API endpoint.\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, from_json, explode\n",
    "from pyspark.sql.types import StringType, StructType, StructField, ArrayType, MapType, IntegerType, BooleanType\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"SpaceX Data Pipeline\").getOrCreate()\n",
    "\n",
    "# Initialize logging\n",
    "log_data = []\n",
    "\n",
    "def log_message(message, level=\"INFO\"):\n",
    "    \"\"\"Log messages with timestamp and level\"\"\"\n",
    "    timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_entry = f\"[{timestamp}] [{level}] {message}\"\n",
    "    print(log_entry)\n",
    "    log_data.append(log_entry)\n",
    "\n",
    "def measure_execution_time(func):\n",
    "    \"\"\"Decorator to measure execution time of functions\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        log_message(f\"Execution of {func.__name__} took {execution_time:.2f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@measure_execution_time\n",
    "def fetch_data(url, data_name):\n",
    "    \"\"\"Fetch data from URL and return as JSON\"\"\"\n",
    "    try:\n",
    "        log_message(f\"Fetching {data_name} data from {url}\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise exception for bad status codes\n",
    "        data = response.json()\n",
    "        log_message(f\"Successfully fetched {len(data)} {data_name} records\")\n",
    "        return data\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        log_message(f\"Error fetching {data_name} data: {str(e)}\", \"ERROR\")\n",
    "        raise\n",
    "\n",
    "@measure_execution_time\n",
    "def convert_to_dataframe(data, data_name):\n",
    "    \"\"\"Convert JSON data to Spark DataFrame\"\"\"\n",
    "    try:\n",
    "        log_message(f\"Converting {data_name} data to DataFrame\")\n",
    "        df = spark.createDataFrame(data)\n",
    "        log_message(f\"Successfully created DataFrame with {df.count()} rows and {len(df.columns)} columns\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error converting {data_name} data to DataFrame: {str(e)}\", \"ERROR\")\n",
    "        raise\n",
    "\n",
    "@measure_execution_time\n",
    "def join_data(launches_df, rockets_df):\n",
    "    \"\"\"Join launches with corresponding rocket data\"\"\"\n",
    "    try:\n",
    "        log_message(\"Joining launch data with rocket names\")\n",
    "        \n",
    "        # Select only needed columns from rockets dataframe\n",
    "        rockets_slim = rockets_df.select(col(\"rocket_id\"), col(\"rocket_name\"))\n",
    "        \n",
    "        # Join launches with rockets on rocket_id\n",
    "        joined_df = launches_df.join(rockets_slim, launches_df.rocket.rocket_id == rockets_slim.rocket_id, \"left\")\n",
    "        \n",
    "        # Log success details\n",
    "        row_count = joined_df.count()\n",
    "        log_message(f\"Successfully joined data, resulting in {row_count} records\")\n",
    "        \n",
    "        return joined_df\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error joining data: {str(e)}\", \"ERROR\")\n",
    "        raise\n",
    "\n",
    "@measure_execution_time\n",
    "def send_data_to_endpoint(data, url):\n",
    "    \"\"\"Send processed data to the specified endpoint\"\"\"\n",
    "    try:\n",
    "        log_message(f\"Sending data to {url}\")\n",
    "        \n",
    "        # Convert DataFrame to JSON format\n",
    "        json_data = data.toJSON().collect()\n",
    "        payload = {\"data\": json_data}\n",
    "        \n",
    "        # Send POST request to endpoint\n",
    "        response = requests.post(url, json=payload)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Log response details\n",
    "        log_message(f\"Successfully sent data. Response status: {response.status_code}\")\n",
    "        log_message(f\"Response content: {response.text[:200]}...\")\n",
    "        \n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        log_message(f\"Error sending data: {str(e)}\", \"ERROR\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main pipeline function\"\"\"\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    log_message(\"Starting SpaceX data pipeline\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Fetch data from APIs\n",
    "        launches_data = fetch_data(\"https://api.spacexdata.com/v3/launches\", \"launches\")\n",
    "        rockets_data = fetch_data(\"https://api.spacexdata.com/v3/rockets\", \"rockets\")\n",
    "        \n",
    "        # 2. Convert to DataFrames\n",
    "        launches_df = convert_to_dataframe(launches_data, \"launches\")\n",
    "        rockets_df = convert_to_dataframe(rockets_data, \"rockets\")\n",
    "        \n",
    "        # 3. Join data\n",
    "        result_df = join_data(launches_df, rockets_df)\n",
    "        \n",
    "        # 4. Show sample of processed data\n",
    "        log_message(\"Sample of processed data:\")\n",
    "        result_df.select(\"flight_number\", \"mission_name\", \"rocket_name\").show(5)\n",
    "        \n",
    "        # 5. Send data to endpoint\n",
    "        response = send_data_to_endpoint(result_df, \"https://httpbin.org/post\")\n",
    "        \n",
    "        # Pipeline completed successfully\n",
    "        total_execution_time = time.time() - total_start_time\n",
    "        log_message(f\"Pipeline completed successfully in {total_execution_time:.2f} seconds\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Pipeline failed: {str(e)}\", \"ERROR\")\n",
    "        raise\n",
    "    finally:\n",
    "        # Save logs to file\n",
    "        log_df = spark.createDataFrame([(log,) for log in log_data], [\"log_message\"])\n",
    "        log_df.write.mode(\"overwrite\").csv(\"/dbfs/pipeline_logs/spacex_pipeline_logs.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfeaa180-07e4-4551-8db3-9b9702046a5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "First error for claude try to give back the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1573c82b-2e37-4a55-8916-6aa6ac995a42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks SpaceX Data Processing Pipeline\n",
    "# This pipeline fetches SpaceX launch and rocket data, joins them, and posts results to an API endpoint.\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, from_json, explode\n",
    "from pyspark.sql.types import StringType, StructType, StructField, ArrayType, MapType, IntegerType, BooleanType, FloatType, TimestampType, DateType, LongType\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"SpaceX Data Pipeline\").getOrCreate()\n",
    "\n",
    "# Initialize logging\n",
    "log_data = []\n",
    "\n",
    "def log_message(message, level=\"INFO\"):\n",
    "    \"\"\"Log messages with timestamp and level\"\"\"\n",
    "    timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_entry = f\"[{timestamp}] [{level}] {message}\"\n",
    "    print(log_entry)\n",
    "    log_data.append(log_entry)\n",
    "\n",
    "def measure_execution_time(func):\n",
    "    \"\"\"Decorator to measure execution time of functions\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        log_message(f\"Execution of {func.__name__} took {execution_time:.2f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@measure_execution_time\n",
    "def fetch_data(url, data_name):\n",
    "    \"\"\"Fetch data from URL and return as JSON\"\"\"\n",
    "    try:\n",
    "        log_message(f\"Fetching {data_name} data from {url}\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise exception for bad status codes\n",
    "        data = response.json()\n",
    "        log_message(f\"Successfully fetched {len(data)} {data_name} records\")\n",
    "        return data\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        log_message(f\"Error fetching {data_name} data: {str(e)}\", \"ERROR\")\n",
    "        raise\n",
    "\n",
    "@measure_execution_time\n",
    "def convert_to_dataframe(data, data_name):\n",
    "    \"\"\"Convert JSON data to Spark DataFrame with explicit schema handling\"\"\"\n",
    "    try:\n",
    "        log_message(f\"Converting {data_name} data to DataFrame\")\n",
    "        \n",
    "        if data_name == \"launches\":\n",
    "            # Define schema for nested fields in launches data\n",
    "            # Create a Spark DataFrame from the JSON string representation\n",
    "            json_data = json.dumps(data)\n",
    "            df = spark.read.json(spark.sparkContext.parallelize([json_data]))\n",
    "            \n",
    "            # Alternative approach with explicit schema if needed\n",
    "            # df = spark.createDataFrame(json.loads(json_data))\n",
    "            \n",
    "        else:  # rockets data has simpler structure\n",
    "            df = spark.createDataFrame(data)\n",
    "            \n",
    "        log_message(f\"Successfully created DataFrame with {df.count()} rows and {len(df.columns)} columns\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error converting {data_name} data to DataFrame: {str(e)}\", \"ERROR\")\n",
    "        raise\n",
    "\n",
    "@measure_execution_time\n",
    "def join_data(launches_df, rockets_df):\n",
    "    \"\"\"Join launches with corresponding rocket data\"\"\"\n",
    "    try:\n",
    "        log_message(\"Joining launch data with rocket names\")\n",
    "        \n",
    "        # Select only needed columns from rockets dataframe\n",
    "        rockets_slim = rockets_df.select(col(\"rocket_id\"), col(\"rocket_name\"))\n",
    "        \n",
    "        # Extract rocket_id from the rocket field in launches_df\n",
    "        # The rocket field contains a struct with rocket_id\n",
    "        launches_with_rocket_id = launches_df.select(\n",
    "            \"*\", \n",
    "            col(\"rocket.rocket_id\").alias(\"launch_rocket_id\")\n",
    "        )\n",
    "        \n",
    "        # Join launches with rockets on rocket_id\n",
    "        joined_df = launches_with_rocket_id.join(\n",
    "            rockets_slim, \n",
    "            launches_with_rocket_id.launch_rocket_id == rockets_slim.rocket_id, \n",
    "            \"left\"\n",
    "        )\n",
    "        \n",
    "        # Log success details\n",
    "        row_count = joined_df.count()\n",
    "        log_message(f\"Successfully joined data, resulting in {row_count} records\")\n",
    "        \n",
    "        return joined_df\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error joining data: {str(e)}\", \"ERROR\")\n",
    "        raise\n",
    "\n",
    "@measure_execution_time\n",
    "def send_data_to_endpoint(data, url):\n",
    "    \"\"\"Send processed data to the specified endpoint\"\"\"\n",
    "    try:\n",
    "        log_message(f\"Sending data to {url}\")\n",
    "        \n",
    "        # Convert DataFrame to JSON format\n",
    "        # Using collect to get all data to driver node - be careful with large datasets\n",
    "        json_data = data.toJSON().collect()\n",
    "        payload = {\"data\": json_data}\n",
    "        \n",
    "        # Send POST request to endpoint\n",
    "        response = requests.post(url, json=payload)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Log response details\n",
    "        log_message(f\"Successfully sent data. Response status: {response.status_code}\")\n",
    "        log_message(f\"Response content: {response.text[:200]}...\")\n",
    "        \n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        log_message(f\"Error sending data: {str(e)}\", \"ERROR\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main pipeline function\"\"\"\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    log_message(\"Starting SpaceX data pipeline\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Fetch data from APIs\n",
    "        launches_data = fetch_data(\"https://api.spacexdata.com/v3/launches\", \"launches\")\n",
    "        rockets_data = fetch_data(\"https://api.spacexdata.com/v3/rockets\", \"rockets\")\n",
    "        \n",
    "        # 2. Convert to DataFrames\n",
    "        launches_df = convert_to_dataframe(launches_data, \"launches\")\n",
    "        rockets_df = convert_to_dataframe(rockets_data, \"rockets\")\n",
    "        \n",
    "        # 3. Print schema to help debugging\n",
    "        log_message(\"Launches DataFrame Schema:\")\n",
    "        launches_df.printSchema()\n",
    "        \n",
    "        log_message(\"Rockets DataFrame Schema:\")\n",
    "        rockets_df.printSchema()\n",
    "        \n",
    "        # 4. Join data\n",
    "        result_df = join_data(launches_df, rockets_df)\n",
    "        \n",
    "        # 5. Show sample of processed data\n",
    "        log_message(\"Sample of processed data:\")\n",
    "        result_df.select(\"flight_number\", \"mission_name\", \"rocket_name\").show(5)\n",
    "        \n",
    "        # 6. Send data to endpoint\n",
    "        response = send_data_to_endpoint(result_df, \"https://httpbin.org/post\")\n",
    "        \n",
    "        # Pipeline completed successfully\n",
    "        total_execution_time = time.time() - total_start_time\n",
    "        log_message(f\"Pipeline completed successfully in {total_execution_time:.2f} seconds\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Pipeline failed: {str(e)}\", \"ERROR\")\n",
    "        raise\n",
    "    finally:\n",
    "        # Save logs to file\n",
    "        log_df = spark.createDataFrame([(log,) for log in log_data], [\"log_message\"])\n",
    "        log_df.write.mode(\"overwrite\").csv(\"/dbfs/pipeline_logs/spacex_pipeline_logs.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07b825e9-6376-486d-9d1c-74685e616293",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "another error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c83e522-c8a4-4d70-87c8-3d1412c28244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks SpaceX Data Processing Pipeline\n",
    "# This pipeline fetches SpaceX launch and rocket data, joins them, and posts results to an API endpoint.\n",
    "# Compatible with Spark Connect / Databricks Serverless\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Initialize SparkSession (already exists in Databricks notebooks)\n",
    "spark = SparkSession.builder.appName(\"SpaceX Data Pipeline\").getOrCreate()\n",
    "\n",
    "# Initialize logging\n",
    "log_data = []\n",
    "\n",
    "def log_message(message, level=\"INFO\"):\n",
    "    \"\"\"Log messages with timestamp and level\"\"\"\n",
    "    timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_entry = f\"[{timestamp}] [{level}] {message}\"\n",
    "    print(log_entry)\n",
    "    log_data.append(log_entry)\n",
    "\n",
    "def measure_execution_time(func):\n",
    "    \"\"\"Decorator to measure execution time of functions\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        log_message(f\"Execution of {func.__name__} took {execution_time:.2f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@measure_execution_time\n",
    "def fetch_data(url, data_name):\n",
    "    \"\"\"Fetch data from URL and return as JSON\"\"\"\n",
    "    try:\n",
    "        log_message(f\"Fetching {data_name} data from {url}\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise exception for bad status codes\n",
    "        data = response.json()\n",
    "        log_message(f\"Successfully fetched {len(data)} {data_name} records\")\n",
    "        return data\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        log_message(f\"Error fetching {data_name} data: {str(e)}\", \"ERROR\")\n",
    "        raise\n",
    "\n",
    "@measure_execution_time\n",
    "def convert_to_dataframe(data, data_name):\n",
    "    \"\"\"Convert JSON data to Spark DataFrame using pandas as intermediate step\"\"\"\n",
    "    try:\n",
    "        log_message(f\"Converting {data_name} data to DataFrame\")\n",
    "        \n",
    "        # First convert to pandas DataFrame (which handles nested JSON well)\n",
    "        pdf = pd.DataFrame(data)\n",
    "        \n",
    "        # For launches data, we need to handle the nested structures\n",
    "        if data_name == \"launches\":\n",
    "            # Extract rocket_id from the nested structure and add as a separate column\n",
    "            pdf['rocket_id'] = pdf['rocket'].apply(lambda x: x.get('rocket_id') if isinstance(x, dict) else None)\n",
    "            \n",
    "            # Convert complex nested structures to strings to avoid schema inference issues\n",
    "            for col_name in pdf.columns:\n",
    "                if pdf[col_name].apply(lambda x: isinstance(x, (dict, list))).any():\n",
    "                    pdf[col_name] = pdf[col_name].apply(lambda x: json.dumps(x) if x is not None else None)\n",
    "        \n",
    "        # Convert pandas DataFrame to Spark DataFrame\n",
    "        df = spark.createDataFrame(pdf)\n",
    "        \n",
    "        log_message(f\"Successfully created DataFrame with {df.count()} rows and {len(df.columns)} columns\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error converting {data_name} data to DataFrame: {str(e)}\", \"ERROR\")\n",
    "        raise\n",
    "\n",
    "@measure_execution_time\n",
    "def join_data(launches_df, rockets_df):\n",
    "    \"\"\"Join launches with corresponding rocket data\"\"\"\n",
    "    try:\n",
    "        log_message(\"Joining launch data with rocket names\")\n",
    "        \n",
    "        # Select only needed columns from rockets dataframe\n",
    "        rockets_slim = rockets_df.select(col(\"rocket_id\"), col(\"rocket_name\"))\n",
    "        \n",
    "        # Join launches with rockets on rocket_id\n",
    "        joined_df = launches_df.join(\n",
    "            rockets_slim, \n",
    "            launches_df.rocket_id == rockets_slim.rocket_id, \n",
    "            \"left\"\n",
    "        )\n",
    "        \n",
    "        # Log success details\n",
    "        row_count = joined_df.count()\n",
    "        log_message(f\"Successfully joined data, resulting in {row_count} records\")\n",
    "        \n",
    "        return joined_df\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error joining data: {str(e)}\", \"ERROR\")\n",
    "        raise\n",
    "\n",
    "@measure_execution_time\n",
    "def send_data_to_endpoint(data, url):\n",
    "    \"\"\"Send processed data to the specified endpoint\"\"\"\n",
    "    try:\n",
    "        log_message(f\"Sending data to {url}\")\n",
    "        \n",
    "        # Convert DataFrame to JSON format\n",
    "        # Using toPandas() to get the data back to driver node - be careful with large datasets\n",
    "        pd_df = data.toPandas()\n",
    "        json_data = pd_df.to_json(orient=\"records\")\n",
    "        payload = {\"data\": json.loads(json_data)}\n",
    "        \n",
    "        # Send POST request to endpoint\n",
    "        response = requests.post(url, json=payload)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Log response details\n",
    "        log_message(f\"Successfully sent data. Response status: {response.status_code}\")\n",
    "        log_message(f\"Response content: {response.text[:200]}...\")\n",
    "        \n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        log_message(f\"Error sending data: {str(e)}\", \"ERROR\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main pipeline function\"\"\"\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    log_message(\"Starting SpaceX data pipeline\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Fetch data from APIs\n",
    "        launches_data = fetch_data(\"https://api.spacexdata.com/v3/launches\", \"launches\")\n",
    "        rockets_data = fetch_data(\"https://api.spacexdata.com/v3/rockets\", \"rockets\")\n",
    "        \n",
    "        # 2. Convert to DataFrames\n",
    "        launches_df = convert_to_dataframe(launches_data, \"launches\")\n",
    "        rockets_df = convert_to_dataframe(rockets_data, \"rockets\")\n",
    "        \n",
    "        # 3. Print schema to help debugging\n",
    "        log_message(\"Launches DataFrame Schema:\")\n",
    "        launches_df.printSchema()\n",
    "        \n",
    "        log_message(\"Rockets DataFrame Schema:\")\n",
    "        rockets_df.printSchema()\n",
    "        \n",
    "        # 4. Join data\n",
    "        result_df = join_data(launches_df, rockets_df)\n",
    "        \n",
    "        # 5. Show sample of processed data\n",
    "        log_message(\"Sample of processed data:\")\n",
    "        result_df.select(\"flight_number\", \"mission_name\", \"rocket_name\").show(5)\n",
    "        \n",
    "        # 6. Send data to endpoint\n",
    "        response = send_data_to_endpoint(result_df, \"https://httpbin.org/post\")\n",
    "        \n",
    "        # Pipeline completed successfully\n",
    "        total_execution_time = time.time() - total_start_time\n",
    "        log_message(f\"Pipeline completed successfully in {total_execution_time:.2f} seconds\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Pipeline failed: {str(e)}\", \"ERROR\")\n",
    "        raise\n",
    "    finally:\n",
    "        # Save logs to DataFrame\n",
    "        log_df = spark.createDataFrame([(log,) for log in log_data], [\"log_message\"])\n",
    "        \n",
    "        # Write logs to file - using Delta format which is better supported in Databricks\n",
    "        log_df.write.format(\"delta\").mode(\"overwrite\").save(\"/pipeline_logs/spacex_pipeline_logs\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5be1a7a5-e4a4-41b5-a4e4-7d08b594dc36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "[2025-04-15 12:55:50] [ERROR] Pipeline failed: DataFrame columns must be unique for orient='records'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0a02bca-0829-4696-a40a-640af5837051",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks SpaceX Data Processing Pipeline\n",
    "# This pipeline fetches SpaceX launch and rocket data, joins them, and posts results to an API endpoint.\n",
    "# Compatible with Spark Connect / Databricks Serverless\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize SparkSession (already exists in Databricks notebooks)\n",
    "spark = SparkSession.builder.appName(\"SpaceX Data Pipeline\").getOrCreate()\n",
    "\n",
    "# Initialize logging\n",
    "log_data = []\n",
    "\n",
    "def log_message(message, level=\"INFO\"):\n",
    "    \"\"\"Log messages with timestamp and level\"\"\"\n",
    "    timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_entry = f\"[{timestamp}] [{level}] {message}\"\n",
    "    print(log_entry)\n",
    "    log_data.append(log_entry)\n",
    "\n",
    "def measure_execution_time(func):\n",
    "    \"\"\"Decorator to measure execution time of functions\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        execution_time = end_time - start_time\n",
    "        log_message(f\"Execution of {func.__name__} took {execution_time:.2f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@measure_execution_time\n",
    "def fetch_data(url, data_name):\n",
    "    \"\"\"Fetch data from URL and return as JSON\"\"\"\n",
    "    try:\n",
    "        log_message(f\"Fetching {data_name} data from {url}\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise exception for bad status codes\n",
    "        data = response.json()\n",
    "        log_message(f\"Successfully fetched {len(data)} {data_name} records\")\n",
    "        return data\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        log_message(f\"Error fetching {data_name} data: {str(e)}\", \"ERROR\")\n",
    "        raise\n",
    "\n",
    "@measure_execution_time\n",
    "def convert_to_dataframe(data, data_name):\n",
    "    \"\"\"Convert JSON data to Spark DataFrame using pandas as intermediate step\"\"\"\n",
    "    try:\n",
    "        log_message(f\"Converting {data_name} data to DataFrame\")\n",
    "        \n",
    "        # First convert to pandas DataFrame (which handles nested JSON well)\n",
    "        pdf = pd.DataFrame(data)\n",
    "        \n",
    "        # For launches data, we need to handle the nested structures\n",
    "        if data_name == \"launches\":\n",
    "            # Extract rocket_id from the nested structure and add as a separate column\n",
    "            pdf['rocket_id'] = pdf['rocket'].apply(lambda x: x.get('rocket_id') if isinstance(x, dict) else None)\n",
    "            \n",
    "            # Convert complex nested structures to strings to avoid schema inference issues\n",
    "            for col_name in pdf.columns:\n",
    "                if pdf[col_name].apply(lambda x: isinstance(x, (dict, list))).any():\n",
    "                    pdf[col_name] = pdf[col_name].apply(lambda x: json.dumps(x) if x is not None else None)\n",
    "        \n",
    "        # Convert pandas DataFrame to Spark DataFrame\n",
    "        df = spark.createDataFrame(pdf)\n",
    "        \n",
    "        log_message(f\"Successfully created DataFrame with {df.count()} rows and {len(df.columns)} columns\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error converting {data_name} data to DataFrame: {str(e)}\", \"ERROR\")\n",
    "        raise\n",
    "\n",
    "@measure_execution_time\n",
    "def join_data(launches_df, rockets_df):\n",
    "    \"\"\"Join launches with corresponding rocket data\"\"\"\n",
    "    try:\n",
    "        log_message(\"Joining launch data with rocket names\")\n",
    "        \n",
    "        # Select only needed columns from rockets dataframe and rename rocket_id to avoid duplicate columns\n",
    "        rockets_slim = rockets_df.select(\n",
    "            col(\"rocket_id\").alias(\"r_rocket_id\"), \n",
    "            col(\"rocket_name\")\n",
    "        )\n",
    "        \n",
    "        # Join launches with rockets on rocket_id\n",
    "        joined_df = launches_df.join(\n",
    "            rockets_slim, \n",
    "            launches_df.rocket_id == rockets_slim.r_rocket_id, \n",
    "            \"left\"\n",
    "        )\n",
    "        \n",
    "        # Log success details\n",
    "        row_count = joined_df.count()\n",
    "        log_message(f\"Successfully joined data, resulting in {row_count} records\")\n",
    "        \n",
    "        return joined_df\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error joining data: {str(e)}\", \"ERROR\")\n",
    "        raise\n",
    "\n",
    "@measure_execution_time\n",
    "def send_data_to_endpoint(data, url):\n",
    "    \"\"\"Send processed data to the specified endpoint\"\"\"\n",
    "    try:\n",
    "        log_message(f\"Sending data to {url}\")\n",
    "        \n",
    "        # Select key columns to reduce data size and ensure column uniqueness\n",
    "        # Drop duplicate columns and select only what we need\n",
    "        slim_df = data.select(\n",
    "            \"flight_number\", \n",
    "            \"mission_name\", \n",
    "            \"launch_year\", \n",
    "            \"launch_date_utc\", \n",
    "            \"launch_success\", \n",
    "            \"details\", \n",
    "            \"rocket_id\",  # from launches\n",
    "            \"rocket_name\"  # from rockets\n",
    "        )\n",
    "        \n",
    "        # Convert DataFrame to pandas for JSON serialization\n",
    "        pd_df = slim_df.toPandas()\n",
    "        \n",
    "        # Check for duplicate column names before conversion\n",
    "        if pd_df.columns.duplicated().any():\n",
    "            # Get list of duplicate columns\n",
    "            dupes = pd_df.columns[pd_df.columns.duplicated()].tolist()\n",
    "            log_message(f\"Found duplicate column names: {dupes}\", \"WARNING\")\n",
    "            \n",
    "            # Rename duplicated columns\n",
    "            for i, col_name in enumerate(pd_df.columns):\n",
    "                if col_name in dupes:\n",
    "                    pd_df.rename(columns={col_name: f\"{col_name}_{i}\"}, inplace=True)\n",
    "        \n",
    "        # Convert to JSON for API request\n",
    "        json_data = pd_df.to_json(orient=\"records\")\n",
    "        payload = {\"data\": json.loads(json_data)}\n",
    "        \n",
    "        # Send POST request to endpoint\n",
    "        response = requests.post(url, json=payload)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Log response details\n",
    "        log_message(f\"Successfully sent data. Response status: {response.status_code}\")\n",
    "        log_message(f\"Response content: {response.text[:200]}...\")\n",
    "        \n",
    "        return response\n",
    "    except Exception as e:\n",
    "        log_message(f\"Error sending data: {str(e)}\", \"ERROR\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main pipeline function\"\"\"\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    log_message(\"Starting SpaceX data pipeline\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Fetch data from APIs\n",
    "        launches_data = fetch_data(\"https://api.spacexdata.com/v3/launches\", \"launches\")\n",
    "        rockets_data = fetch_data(\"https://api.spacexdata.com/v3/rockets\", \"rockets\")\n",
    "        \n",
    "        # 2. Convert to DataFrames\n",
    "        launches_df = convert_to_dataframe(launches_data, \"launches\")\n",
    "        rockets_df = convert_to_dataframe(rockets_data, \"rockets\")\n",
    "        \n",
    "        # 3. Print schema to help debugging\n",
    "        log_message(\"Launches DataFrame Schema:\")\n",
    "        launches_df.printSchema()\n",
    "        \n",
    "        log_message(\"Rockets DataFrame Schema:\")\n",
    "        rockets_df.printSchema()\n",
    "        \n",
    "        # 4. Join data\n",
    "        result_df = join_data(launches_df, rockets_df)\n",
    "        \n",
    "        # 5. Show sample of processed data\n",
    "        log_message(\"Sample of processed data:\")\n",
    "        result_df.select(\"flight_number\", \"mission_name\", \"rocket_name\").show(5)\n",
    "        \n",
    "        # 6. Send data to endpoint\n",
    "        response = send_data_to_endpoint(result_df, \"https://httpbin.org/post\")\n",
    "        \n",
    "        # Pipeline completed successfully\n",
    "        total_execution_time = time.time() - total_start_time\n",
    "        log_message(f\"Pipeline completed successfully in {total_execution_time:.2f} seconds\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_message(f\"Pipeline failed: {str(e)}\", \"ERROR\")\n",
    "        raise\n",
    "    finally:\n",
    "        # Save logs to DataFrame\n",
    "        log_df = spark.createDataFrame([(log,) for log in log_data], [\"log_message\"])\n",
    "        \n",
    "        # Write logs to file - using Delta format which is better supported in Databricks\n",
    "        log_df.write.format(\"delta\").mode(\"overwrite\").save(\"/pipeline_logs/spacex_pipeline_logs\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f05e249-5389-4591-963c-20d4d3952438",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Merge Claude 3.7 Sonnet",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
