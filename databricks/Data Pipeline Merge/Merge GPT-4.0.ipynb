{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1137619-5355-4fef-890f-52fe0a9dd7a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## PROMPT\n",
    "Generate a pipeline for Databricks:\n",
    "\n",
    "Get Data information from two web addresses:\n",
    "\n",
    "Space launches: https://api.spacexdata.com/v3/launches\n",
    "\n",
    "Space rockets: https://api.spacexdata.com/v3/rockets\n",
    "\n",
    "Combine each launch record with the name of the corresponding rocket.\n",
    "\n",
    "Send the final result to the web address: https://httpbin.org/post\n",
    "\n",
    "The script must provide status updates on its progress, report any errors encountered, confirm the outcome of the final data sending step, and measure/report execution times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee0d752a-c304-43fb-b7ae-0f6012b395ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session (if running locally or in Databricks)\n",
    "spark = SparkSession.builder.appName(\"SpaceX Launches Pipeline\").getOrCreate()\n",
    "\n",
    "# Start total execution timer\n",
    "start_time = time.time()\n",
    "\n",
    "def log_status(message):\n",
    "    print(f\"[STATUS] {message}\")\n",
    "\n",
    "def log_error(message):\n",
    "    print(f\"[ERROR] {message}\")\n",
    "\n",
    "def fetch_data(url, name):\n",
    "    try:\n",
    "        log_status(f\"Fetching {name} data from {url}...\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        log_status(f\"Successfully fetched {name} data.\")\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        log_error(f\"Failed to fetch {name} data: {e}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    # Step 1: Fetch data\n",
    "    t0 = time.time()\n",
    "    launches_data = fetch_data(\"https://api.spacexdata.com/v3/launches\", \"launches\")\n",
    "    rockets_data = fetch_data(\"https://api.spacexdata.com/v3/rockets\", \"rockets\")\n",
    "    log_status(f\"Data fetched in {time.time() - t0:.2f} seconds.\")\n",
    "\n",
    "    # Step 2: Create DataFrames\n",
    "    t1 = time.time()\n",
    "    launches_df = spark.createDataFrame(launches_data)\n",
    "    rockets_df = spark.createDataFrame(rockets_data)\n",
    "    log_status(\"DataFrames created.\")\n",
    "\n",
    "    # Step 3: Join data\n",
    "    log_status(\"Joining launch data with rocket names...\")\n",
    "    rockets_df = rockets_df.select(col(\"rocket_id\").alias(\"rocket_id_ref\"), col(\"rocket_name\"))\n",
    "    combined_df = launches_df.join(rockets_df, launches_df.rocket.rocket_id == rockets_df.rocket_id_ref, \"left\")\n",
    "    final_df = combined_df.select(\n",
    "        col(\"flight_number\"),\n",
    "        col(\"mission_name\"),\n",
    "        col(\"launch_year\"),\n",
    "        col(\"rocket.rocket_id\"),\n",
    "        col(\"rocket_name\")\n",
    "    )\n",
    "    log_status(f\"Join completed in {time.time() - t1:.2f} seconds.\")\n",
    "\n",
    "    # Step 4: Convert to JSON and send to endpoint\n",
    "    t2 = time.time()\n",
    "    final_json = final_df.toJSON().collect()\n",
    "    response = requests.post(\"https://httpbin.org/post\", json=final_json)\n",
    "    if response.status_code == 200:\n",
    "        log_status(f\"Data successfully sent to endpoint in {time.time() - t2:.2f} seconds.\")\n",
    "    else:\n",
    "        log_error(f\"Failed to send data to endpoint. Status code: {response.status_code}\")\n",
    "\n",
    "except Exception as e:\n",
    "    log_error(f\"Pipeline failed: {e}\")\n",
    "\n",
    "finally:\n",
    "    total_time = time.time() - start_time\n",
    "    log_status(f\"Pipeline execution completed in {total_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b9830f1-25e9-4ba0-b221-bd837b4c95e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "first error for the links field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb25626f-7897-4f50-97b8-d5a3d6d88cc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SpaceX Launches Pipeline\").getOrCreate()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def log_status(message):\n",
    "    print(f\"[STATUS] {message}\")\n",
    "\n",
    "def log_error(message):\n",
    "    print(f\"[ERROR] {message}\")\n",
    "\n",
    "def fetch_data(url, name):\n",
    "    try:\n",
    "        log_status(f\"Fetching {name} data from {url}...\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        log_status(f\"Successfully fetched {name} data.\")\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        log_error(f\"Failed to fetch {name} data: {e}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    # Step 1: Fetch\n",
    "    t0 = time.time()\n",
    "    launches_raw = fetch_data(\"https://api.spacexdata.com/v3/launches\", \"launches\")\n",
    "    rockets_raw = fetch_data(\"https://api.spacexdata.com/v3/rockets\", \"rockets\")\n",
    "    log_status(f\"Data fetched in {time.time() - t0:.2f} seconds.\")\n",
    "\n",
    "    # Step 2: Simplify launches data (keep only necessary fields)\n",
    "    launches_cleaned = []\n",
    "    for launch in launches_raw:\n",
    "        launches_cleaned.append({\n",
    "            \"flight_number\": launch.get(\"flight_number\"),\n",
    "            \"mission_name\": launch.get(\"mission_name\"),\n",
    "            \"launch_year\": launch.get(\"launch_year\"),\n",
    "            \"rocket_id\": launch.get(\"rocket\", {}).get(\"rocket_id\")\n",
    "        })\n",
    "\n",
    "    # Simplify rockets data\n",
    "    rockets_cleaned = []\n",
    "    for rocket in rockets_raw:\n",
    "        rockets_cleaned.append({\n",
    "            \"rocket_id\": rocket.get(\"rocket_id\"),\n",
    "            \"rocket_name\": rocket.get(\"rocket_name\")\n",
    "        })\n",
    "\n",
    "    # Step 3: Create DataFrames\n",
    "    t1 = time.time()\n",
    "    launches_df = spark.createDataFrame(launches_cleaned)\n",
    "    rockets_df = spark.createDataFrame(rockets_cleaned)\n",
    "    log_status(\"Cleaned DataFrames created.\")\n",
    "\n",
    "    # Step 4: Join\n",
    "    combined_df = launches_df.join(rockets_df, on=\"rocket_id\", how=\"left\")\n",
    "    log_status(f\"Join completed in {time.time() - t1:.2f} seconds.\")\n",
    "\n",
    "    # Step 5: Send result\n",
    "    t2 = time.time()\n",
    "    final_json = combined_df.toJSON().collect()\n",
    "    response = requests.post(\"https://httpbin.org/post\", json=final_json)\n",
    "    if response.status_code == 200:\n",
    "        log_status(f\"Data successfully sent to endpoint in {time.time() - t2:.2f} seconds.\")\n",
    "    else:\n",
    "        log_error(f\"Failed to send data to endpoint. Status code: {response.status_code}\")\n",
    "\n",
    "except Exception as e:\n",
    "    log_error(f\"Pipeline failed: {e}\")\n",
    "\n",
    "finally:\n",
    "    total_time = time.time() - start_time\n",
    "    log_status(f\"Pipeline execution completed in {total_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76e64119-e95a-436a-a412-6c314075b0a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SpaceX Launches Pipeline\").getOrCreate()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def log_status(message):\n",
    "    print(f\"[STATUS] {message}\")\n",
    "\n",
    "def log_error(message):\n",
    "    print(f\"[ERROR] {message}\")\n",
    "\n",
    "def fetch_data(url, name):\n",
    "    try:\n",
    "        log_status(f\"Fetching {name} data from {url}...\")\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        log_status(f\"Successfully fetched {name} data.\")\n",
    "        return response.json()\n",
    "    except Exception as e:\n",
    "        log_error(f\"Failed to fetch {name} data: {e}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    # Step 1: Fetch\n",
    "    t0 = time.time()\n",
    "    launches_raw = fetch_data(\"https://api.spacexdata.com/v3/launches\", \"launches\")\n",
    "    rockets_raw = fetch_data(\"https://api.spacexdata.com/v3/rockets\", \"rockets\")\n",
    "    log_status(f\"Data fetched in {time.time() - t0:.2f} seconds.\")\n",
    "\n",
    "    # Step 2: Clean/Extract\n",
    "    launches_cleaned = [{\n",
    "        \"flight_number\": l.get(\"flight_number\"),\n",
    "        \"mission_name\": l.get(\"mission_name\"),\n",
    "        \"launch_year\": l.get(\"launch_year\"),\n",
    "        \"rocket_id\": l.get(\"rocket\", {}).get(\"rocket_id\")\n",
    "    } for l in launches_raw]\n",
    "\n",
    "    rockets_cleaned = [{\n",
    "        \"rocket_id\": r.get(\"rocket_id\"),\n",
    "        \"rocket_name\": r.get(\"rocket_name\")\n",
    "    } for r in rockets_raw]\n",
    "\n",
    "    # Step 3: Create DataFrames\n",
    "    t1 = time.time()\n",
    "    launches_df = spark.createDataFrame(launches_cleaned)\n",
    "    rockets_df = spark.createDataFrame(rockets_cleaned)\n",
    "    log_status(\"Cleaned DataFrames created.\")\n",
    "\n",
    "    # Step 4: Join\n",
    "    combined_df = launches_df.join(rockets_df, on=\"rocket_id\", how=\"left\")\n",
    "    log_status(f\"Join completed in {time.time() - t1:.2f} seconds.\")\n",
    "\n",
    "    # Step 5: Convert to JSON and Send\n",
    "    t2 = time.time()\n",
    "    result_json = combined_df.toPandas().to_dict(orient=\"records\")\n",
    "    response = requests.post(\"https://httpbin.org/post\", json=result_json)\n",
    "    if response.status_code == 200:\n",
    "        log_status(f\"Data successfully sent to endpoint in {time.time() - t2:.2f} seconds.\")\n",
    "    else:\n",
    "        log_error(f\"Failed to send data to endpoint. Status code: {response.status_code}\")\n",
    "\n",
    "except Exception as e:\n",
    "    log_error(f\"Pipeline failed: {e}\")\n",
    "\n",
    "finally:\n",
    "    total_time = time.time() - start_time\n",
    "    log_status(f\"Pipeline execution completed in {total_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4f119f0-5f9f-403d-a2d3-33f3ba319fc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Merge GPT-4.0",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
