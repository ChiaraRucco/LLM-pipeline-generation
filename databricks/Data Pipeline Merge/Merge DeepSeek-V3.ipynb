{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1137619-5355-4fef-890f-52fe0a9dd7a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## PROMPT\n",
    "Generate a pipeline for Databricks:\n",
    "\n",
    "Get Data information from two web addresses:\n",
    "\n",
    "Space launches: https://api.spacexdata.com/v3/launches\n",
    "\n",
    "Space rockets: https://api.spacexdata.com/v3/rockets\n",
    "\n",
    "Combine each launch record with the name of the corresponding rocket.\n",
    "\n",
    "Send the final result to the web address: https://httpbin.org/post\n",
    "\n",
    "The script must provide status updates on its progress, report any errors encountered, confirm the outcome of the final data sending step, and measure/report execution times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "717c8236-086e-433c-954a-421d2db8b02d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def log_status(message):\n",
    "    \"\"\"Helper function to log status messages with timestamp\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"[{timestamp}] {message}\")\n",
    "\n",
    "def fetch_data(url, description):\n",
    "    \"\"\"Fetch data from API with error handling\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        log_status(f\"Fetching {description} from {url}\")\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raises exception for 4XX/5XX errors\n",
    "        \n",
    "        data = response.json()\n",
    "        elapsed = time.time() - start_time\n",
    "        log_status(f\"Successfully fetched {len(data) if isinstance(data, list) else 1} {description} records in {elapsed:.2f} seconds\")\n",
    "        return data\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        log_status(f\"Error fetching {description}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def send_data(url, data):\n",
    "    \"\"\"Send data to endpoint with error handling\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        log_status(f\"Sending combined data to {url}\")\n",
    "        \n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        response = requests.post(url, data=json.dumps(data), headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        log_status(f\"Data successfully sent in {elapsed:.2f} seconds\")\n",
    "        log_status(f\"Response status: {response.status_code}\")\n",
    "        log_status(f\"Response content: {response.text[:200]}...\")  # Truncate long responses\n",
    "        return True\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        log_status(f\"Error sending data: {str(e)}\")\n",
    "        if hasattr(e, 'response') and e.response:\n",
    "            log_status(f\"Response content: {e.response.text[:200]}...\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    # Record total pipeline execution time\n",
    "    pipeline_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Initialize Spark session\n",
    "        spark = SparkSession.builder.appName(\"SpaceXDataPipeline\").getOrCreate()\n",
    "        \n",
    "        # 1. Fetch data from APIs\n",
    "        launches_url = \"https://api.spacexdata.com/v3/launches\"\n",
    "        rockets_url = \"https://api.spacexdata.com/v3/rockets\"\n",
    "        \n",
    "        launches_data = fetch_data(launches_url, \"launches\")\n",
    "        rockets_data = fetch_data(rockets_url, \"rockets\")\n",
    "        \n",
    "        # 2. Process and combine data\n",
    "        log_status(\"Starting data processing...\")\n",
    "        process_start = time.time()\n",
    "        \n",
    "        # Create DataFrames\n",
    "        launches_df = spark.createDataFrame(launches_data)\n",
    "        rockets_df = spark.createDataFrame(rockets_data)\n",
    "        \n",
    "        # Select only needed columns from rockets\n",
    "        rockets_df = rockets_df.select(col(\"rocket_id\"), col(\"rocket_name\"))\n",
    "        \n",
    "        # Join launches with rocket names\n",
    "        combined_df = launches_df.join(rockets_df, \n",
    "                                     launches_df.rocket.rocket_id == rockets_df.rocket_id,\n",
    "                                     \"left\")\n",
    "        \n",
    "        # Drop the redundant rocket_id column\n",
    "        combined_df = combined_df.drop(\"rocket_id\")\n",
    "        \n",
    "        # Convert back to JSON for sending\n",
    "        combined_data = [row.asDict() for row in combined_df.collect()]\n",
    "        \n",
    "        process_elapsed = time.time() - process_start\n",
    "        log_status(f\"Data processing completed in {process_elapsed:.2f} seconds\")\n",
    "        log_status(f\"Combined {len(combined_data)} launch records with rocket names\")\n",
    "        \n",
    "        # 3. Send data to endpoint\n",
    "        target_url = \"https://httpbin.org/post\"\n",
    "        send_success = send_data(target_url, combined_data)\n",
    "        \n",
    "        if send_success:\n",
    "            log_status(\"Pipeline completed successfully\")\n",
    "        else:\n",
    "            log_status(\"Pipeline completed with errors in sending step\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        log_status(f\"Pipeline failed with error: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        pipeline_elapsed = time.time() - pipeline_start\n",
    "        log_status(f\"Total pipeline execution time: {pipeline_elapsed:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fd17c9b-81e1-4660-b2ca-e2dbcbfec89a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "DeepSeek need a correction of the code, so we try to give the output back to the LLM to write again the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cbafc80-3b17-4ac2-a0ec-7ab046613cc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, MapType, ArrayType\n",
    "\n",
    "def log_status(message):\n",
    "    \"\"\"Helper function to log status messages with timestamp\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"[{timestamp}] {message}\")\n",
    "\n",
    "def fetch_data(url, description):\n",
    "    \"\"\"Fetch data from API with error handling\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        log_status(f\"Fetching {description} from {url}\")\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raises exception for 4XX/5XX errors\n",
    "        \n",
    "        data = response.json()\n",
    "        elapsed = time.time() - start_time\n",
    "        log_status(f\"Successfully fetched {len(data) if isinstance(data, list) else 1} {description} records in {elapsed:.2f} seconds\")\n",
    "        return data\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        log_status(f\"Error fetching {description}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def send_data(url, data):\n",
    "    \"\"\"Send data to endpoint with error handling\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        log_status(f\"Sending combined data to {url}\")\n",
    "        \n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        response = requests.post(url, data=json.dumps(data), headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        log_status(f\"Data successfully sent in {elapsed:.2f} seconds\")\n",
    "        log_status(f\"Response status: {response.status_code}\")\n",
    "        log_status(f\"Response content: {response.text[:200]}...\")  # Truncate long responses\n",
    "        return True\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        log_status(f\"Error sending data: {str(e)}\")\n",
    "        if hasattr(e, 'response') and e.response:\n",
    "            log_status(f\"Response content: {e.response.text[:200]}...\")\n",
    "        return False\n",
    "\n",
    "def get_launches_schema():\n",
    "    \"\"\"Define schema for launches data to handle complex nested structures\"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"flight_number\", IntegerType(), True),\n",
    "        StructField(\"mission_name\", StringType(), True),\n",
    "        StructField(\"launch_year\", StringType(), True),\n",
    "        StructField(\"launch_date_utc\", StringType(), True),\n",
    "        StructField(\"rocket\", StructType([\n",
    "            StructField(\"rocket_id\", StringType(), True),\n",
    "            StructField(\"rocket_name\", StringType(), True),\n",
    "            StructField(\"rocket_type\", StringType(), True)\n",
    "        ]), True),\n",
    "        StructField(\"launch_success\", BooleanType(), True),\n",
    "        StructField(\"links\", MapType(StringType(), StringType()), True),\n",
    "        StructField(\"details\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "def get_rockets_schema():\n",
    "    \"\"\"Define schema for rockets data\"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"rocket_id\", StringType(), True),\n",
    "        StructField(\"rocket_name\", StringType(), True),\n",
    "        StructField(\"rocket_type\", StringType(), True),\n",
    "        StructField(\"active\", BooleanType(), True),\n",
    "        StructField(\"cost_per_launch\", IntegerType(), True),\n",
    "        StructField(\"success_rate_pct\", IntegerType(), True),\n",
    "        StructField(\"first_flight\", StringType(), True),\n",
    "        StructField(\"country\", StringType(), True),\n",
    "        StructField(\"company\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "def transform_launch_data(launch):\n",
    "    \"\"\"Transform launch data to match our schema\"\"\"\n",
    "    return {\n",
    "        \"flight_number\": launch.get(\"flight_number\"),\n",
    "        \"mission_name\": launch.get(\"mission_name\"),\n",
    "        \"launch_year\": launch.get(\"launch_year\"),\n",
    "        \"launch_date_utc\": launch.get(\"launch_date_utc\"),\n",
    "        \"rocket\": {\n",
    "            \"rocket_id\": launch.get(\"rocket\", {}).get(\"rocket_id\"),\n",
    "            \"rocket_name\": None,  # Will be filled in join\n",
    "            \"rocket_type\": launch.get(\"rocket\", {}).get(\"rocket_type\")\n",
    "        },\n",
    "        \"launch_success\": launch.get(\"launch_success\"),\n",
    "        \"links\": launch.get(\"links\", {}),\n",
    "        \"details\": launch.get(\"details\")\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # Record total pipeline execution time\n",
    "    pipeline_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Initialize Spark session\n",
    "        spark = SparkSession.builder.appName(\"SpaceXDataPipeline\").getOrCreate()\n",
    "        \n",
    "        # 1. Fetch data from APIs\n",
    "        launches_url = \"https://api.spacexdata.com/v3/launches\"\n",
    "        rockets_url = \"https://api.spacexdata.com/v3/rockets\"\n",
    "        \n",
    "        launches_data = fetch_data(launches_url, \"launches\")\n",
    "        rockets_data = fetch_data(rockets_url, \"rockets\")\n",
    "        \n",
    "        # 2. Process and combine data\n",
    "        log_status(\"Starting data processing...\")\n",
    "        process_start = time.time()\n",
    "        \n",
    "        # Transform data to match schemas\n",
    "        transformed_launches = [transform_launch_data(launch) for launch in launches_data]\n",
    "        \n",
    "        # Create DataFrames with explicit schemas\n",
    "        launches_df = spark.createDataFrame(transformed_launches, schema=get_launches_schema())\n",
    "        rockets_df = spark.createDataFrame(rockets_data, schema=get_rockets_schema())\n",
    "        \n",
    "        # Select only needed columns from rockets\n",
    "        rockets_df = rockets_df.select(\n",
    "            col(\"rocket_id\"), \n",
    "            col(\"rocket_name\").alias(\"rocket_name_from_join\"),\n",
    "            col(\"rocket_type\")\n",
    "        )\n",
    "        \n",
    "        # Join launches with rocket names\n",
    "        combined_df = launches_df.join(\n",
    "            rockets_df, \n",
    "            launches_df.rocket.rocket_id == rockets_df.rocket_id,\n",
    "            \"left\"\n",
    "        )\n",
    "        \n",
    "        # Update the rocket_name in the nested struct\n",
    "        combined_df = combined_df.withColumn(\n",
    "            \"rocket\",\n",
    "            col(\"rocket\").withField(\n",
    "                \"rocket_name\",\n",
    "                col(\"rocket_name_from_join\")\n",
    "            )\n",
    "        ).drop(\"rocket_name_from_join\", \"rocket_id\")\n",
    "        \n",
    "        # Convert back to JSON for sending\n",
    "        combined_data = [json.loads(row.json()) for row in combined_df.collect()]\n",
    "        \n",
    "        process_elapsed = time.time() - process_start\n",
    "        log_status(f\"Data processing completed in {process_elapsed:.2f} seconds\")\n",
    "        log_status(f\"Combined {len(combined_data)} launch records with rocket names\")\n",
    "        \n",
    "        # 3. Send data to endpoint\n",
    "        target_url = \"https://httpbin.org/post\"\n",
    "        send_success = send_data(target_url, combined_data)\n",
    "        \n",
    "        if send_success:\n",
    "            log_status(\"Pipeline completed successfully\")\n",
    "        else:\n",
    "            log_status(\"Pipeline completed with errors in sending step\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        log_status(f\"Pipeline failed with error: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        pipeline_elapsed = time.time() - pipeline_start\n",
    "        log_status(f\"Total pipeline execution time: {pipeline_elapsed:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b6a2fd6-26f5-4d5b-8609-361e4324c439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Another error appear so i try again to give back to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6d353b1-06bb-4931-b61d-f6e079a88d02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, MapType\n",
    "\n",
    "def log_status(message):\n",
    "    \"\"\"Helper function to log status messages with timestamp\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"[{timestamp}] {message}\")\n",
    "\n",
    "def fetch_data(url, description):\n",
    "    \"\"\"Fetch data from API with error handling\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        log_status(f\"Fetching {description} from {url}\")\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raises exception for 4XX/5XX errors\n",
    "        \n",
    "        data = response.json()\n",
    "        elapsed = time.time() - start_time\n",
    "        log_status(f\"Successfully fetched {len(data) if isinstance(data, list) else 1} {description} records in {elapsed:.2f} seconds\")\n",
    "        return data\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        log_status(f\"Error fetching {description}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def send_data(url, data):\n",
    "    \"\"\"Send data to endpoint with error handling\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        log_status(f\"Sending combined data to {url}\")\n",
    "        \n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        response = requests.post(url, data=json.dumps(data), headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        log_status(f\"Data successfully sent in {elapsed:.2f} seconds\")\n",
    "        log_status(f\"Response status: {response.status_code}\")\n",
    "        log_status(f\"Response content: {response.text[:200]}...\")  # Truncate long responses\n",
    "        return True\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        log_status(f\"Error sending data: {str(e)}\")\n",
    "        if hasattr(e, 'response') and e.response:\n",
    "            log_status(f\"Response content: {e.response.text[:200]}...\")\n",
    "        return False\n",
    "\n",
    "def get_launches_schema():\n",
    "    \"\"\"Define schema for launches data to handle complex nested structures\"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"flight_number\", IntegerType(), True),\n",
    "        StructField(\"mission_name\", StringType(), True),\n",
    "        StructField(\"launch_year\", StringType(), True),\n",
    "        StructField(\"launch_date_utc\", StringType(), True),\n",
    "        StructField(\"rocket\", StructType([\n",
    "            StructField(\"rocket_id\", StringType(), True),\n",
    "            StructField(\"rocket_name\", StringType(), True),\n",
    "            StructField(\"rocket_type\", StringType(), True)\n",
    "        ]), True),\n",
    "        StructField(\"launch_success\", BooleanType(), True),\n",
    "        StructField(\"links\", MapType(StringType(), StringType()), True),\n",
    "        StructField(\"details\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "def get_rockets_schema():\n",
    "    \"\"\"Define schema for rockets data\"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"rocket_id\", StringType(), True),\n",
    "        StructField(\"rocket_name\", StringType(), True),\n",
    "        StructField(\"rocket_type\", StringType(), True),\n",
    "        StructField(\"active\", BooleanType(), True),\n",
    "        StructField(\"cost_per_launch\", IntegerType(), True),\n",
    "        StructField(\"success_rate_pct\", IntegerType(), True),\n",
    "        StructField(\"first_flight\", StringType(), True),\n",
    "        StructField(\"country\", StringType(), True),\n",
    "        StructField(\"company\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "def transform_launch_data(launch):\n",
    "    \"\"\"Transform launch data to match our schema\"\"\"\n",
    "    # Handle the links field which contains complex nested structures\n",
    "    links = launch.get(\"links\", {})\n",
    "    simplified_links = {}\n",
    "    \n",
    "    # Convert all link values to strings\n",
    "    for key, value in links.items():\n",
    "        if isinstance(value, dict):\n",
    "            # Handle nested dictionaries (like mission_patch_small)\n",
    "            simplified_links[key] = json.dumps(value)\n",
    "        elif value is None:\n",
    "            simplified_links[key] = \"\"\n",
    "        else:\n",
    "            simplified_links[key] = str(value)\n",
    "    \n",
    "    return {\n",
    "        \"flight_number\": launch.get(\"flight_number\"),\n",
    "        \"mission_name\": launch.get(\"mission_name\"),\n",
    "        \"launch_year\": launch.get(\"launch_year\"),\n",
    "        \"launch_date_utc\": launch.get(\"launch_date_utc\"),\n",
    "        \"rocket\": {\n",
    "            \"rocket_id\": launch.get(\"rocket\", {}).get(\"rocket_id\"),\n",
    "            \"rocket_name\": None,  # Will be filled in join\n",
    "            \"rocket_type\": launch.get(\"rocket\", {}).get(\"rocket_type\")\n",
    "        },\n",
    "        \"launch_success\": launch.get(\"launch_success\"),\n",
    "        \"links\": simplified_links,\n",
    "        \"details\": launch.get(\"details\", \"\")\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # Record total pipeline execution time\n",
    "    pipeline_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Initialize Spark session\n",
    "        spark = SparkSession.builder.appName(\"SpaceXDataPipeline\").getOrCreate()\n",
    "        \n",
    "        # 1. Fetch data from APIs\n",
    "        launches_url = \"https://api.spacexdata.com/v3/launches\"\n",
    "        rockets_url = \"https://api.spacexdata.com/v3/rockets\"\n",
    "        \n",
    "        launches_data = fetch_data(launches_url, \"launches\")\n",
    "        rockets_data = fetch_data(rockets_url, \"rockets\")\n",
    "        \n",
    "        # 2. Process and combine data\n",
    "        log_status(\"Starting data processing...\")\n",
    "        process_start = time.time()\n",
    "        \n",
    "        # Transform data to match schemas\n",
    "        transformed_launches = [transform_launch_data(launch) for launch in launches_data]\n",
    "        \n",
    "        # Create DataFrames with explicit schemas\n",
    "        launches_df = spark.createDataFrame(transformed_launches, schema=get_launches_schema())\n",
    "        rockets_df = spark.createDataFrame(rockets_data, schema=get_rockets_schema())\n",
    "        \n",
    "        # Select only needed columns from rockets\n",
    "        rockets_df = rockets_df.select(\n",
    "            col(\"rocket_id\"), \n",
    "            col(\"rocket_name\").alias(\"rocket_name_from_join\"),\n",
    "            col(\"rocket_type\")\n",
    "        )\n",
    "        \n",
    "        # Join launches with rocket names\n",
    "        combined_df = launches_df.join(\n",
    "            rockets_df, \n",
    "            launches_df.rocket.rocket_id == rockets_df.rocket_id,\n",
    "            \"left\"\n",
    "        )\n",
    "        \n",
    "        # Update the rocket_name in the nested struct\n",
    "        combined_df = combined_df.withColumn(\n",
    "            \"rocket\",\n",
    "            col(\"rocket\").withField(\n",
    "                \"rocket_name\",\n",
    "                col(\"rocket_name_from_join\")\n",
    "            )\n",
    "        ).drop(\"rocket_name_from_join\", \"rocket_id\")\n",
    "        \n",
    "        # Convert DataFrame to JSON properly\n",
    "        combined_data = [json.loads(row) for row in combined_df.toJSON().collect()]\n",
    "        \n",
    "        process_elapsed = time.time() - process_start\n",
    "        log_status(f\"Data processing completed in {process_elapsed:.2f} seconds\")\n",
    "        log_status(f\"Combined {len(combined_data)} launch records with rocket names\")\n",
    "        \n",
    "        # 3. Send data to endpoint\n",
    "        target_url = \"https://httpbin.org/post\"\n",
    "        send_success = send_data(target_url, combined_data)\n",
    "        \n",
    "        if send_success:\n",
    "            log_status(\"Pipeline completed successfully\")\n",
    "        else:\n",
    "            log_status(\"Pipeline completed with errors in sending step\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        log_status(f\"Pipeline failed with error: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        pipeline_elapsed = time.time() - pipeline_start\n",
    "        log_status(f\"Total pipeline execution time: {pipeline_elapsed:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9ae8a5e-8b63-477c-9bae-68af97ee825b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "try with the 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19e940a0-dfc0-4972-ad1b-146c361d4463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, struct, to_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, MapType\n",
    "\n",
    "def log_status(message):\n",
    "    \"\"\"Helper function to log status messages with timestamp\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"[{timestamp}] {message}\")\n",
    "\n",
    "def fetch_data(url, description):\n",
    "    \"\"\"Fetch data from API with error handling\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        log_status(f\"Fetching {description} from {url}\")\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        elapsed = time.time() - start_time\n",
    "        log_status(f\"Successfully fetched {len(data) if isinstance(data, list) else 1} {description} records in {elapsed:.2f} seconds\")\n",
    "        return data\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        log_status(f\"Error fetching {description}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def send_data(url, data):\n",
    "    \"\"\"Send data to endpoint with error handling\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        log_status(f\"Sending combined data to {url}\")\n",
    "        \n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        response = requests.post(url, data=json.dumps(data), headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        log_status(f\"Data successfully sent in {elapsed:.2f} seconds\")\n",
    "        log_status(f\"Response status: {response.status_code}\")\n",
    "        log_status(f\"Response content: {response.text[:200]}...\")\n",
    "        return True\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        log_status(f\"Error sending data: {str(e)}\")\n",
    "        if hasattr(e, 'response') and e.response:\n",
    "            log_status(f\"Response content: {e.response.text[:200]}...\")\n",
    "        return False\n",
    "\n",
    "def get_launches_schema():\n",
    "    \"\"\"Define schema for launches data\"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"flight_number\", IntegerType(), True),\n",
    "        StructField(\"mission_name\", StringType(), True),\n",
    "        StructField(\"launch_year\", StringType(), True),\n",
    "        StructField(\"launch_date_utc\", StringType(), True),\n",
    "        StructField(\"rocket\", StructType([\n",
    "            StructField(\"rocket_id\", StringType(), True),\n",
    "            StructField(\"rocket_name\", StringType(), True),\n",
    "            StructField(\"rocket_type\", StringType(), True)\n",
    "        ]), True),\n",
    "        StructField(\"launch_success\", BooleanType(), True),\n",
    "        StructField(\"links\", MapType(StringType(), StringType()), True),\n",
    "        StructField(\"details\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "def get_rockets_schema():\n",
    "    \"\"\"Define schema for rockets data\"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"rocket_id\", StringType(), True),\n",
    "        StructField(\"rocket_name\", StringType(), True),\n",
    "        StructField(\"rocket_type\", StringType(), True),\n",
    "        StructField(\"active\", BooleanType(), True),\n",
    "        StructField(\"cost_per_launch\", IntegerType(), True),\n",
    "        StructField(\"success_rate_pct\", IntegerType(), True),\n",
    "        StructField(\"first_flight\", StringType(), True),\n",
    "        StructField(\"country\", StringType(), True),\n",
    "        StructField(\"company\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "def transform_launch_data(launch):\n",
    "    \"\"\"Transform launch data to match our schema\"\"\"\n",
    "    links = launch.get(\"links\", {})\n",
    "    simplified_links = {}\n",
    "    \n",
    "    for key, value in links.items():\n",
    "        if isinstance(value, dict):\n",
    "            simplified_links[key] = json.dumps(value)\n",
    "        elif value is None:\n",
    "            simplified_links[key] = \"\"\n",
    "        else:\n",
    "            simplified_links[key] = str(value)\n",
    "    \n",
    "    return {\n",
    "        \"flight_number\": launch.get(\"flight_number\"),\n",
    "        \"mission_name\": launch.get(\"mission_name\"),\n",
    "        \"launch_year\": launch.get(\"launch_year\"),\n",
    "        \"launch_date_utc\": launch.get(\"launch_date_utc\"),\n",
    "        \"rocket\": {\n",
    "            \"rocket_id\": launch.get(\"rocket\", {}).get(\"rocket_id\"),\n",
    "            \"rocket_name\": None,\n",
    "            \"rocket_type\": launch.get(\"rocket\", {}).get(\"rocket_type\")\n",
    "        },\n",
    "        \"launch_success\": launch.get(\"launch_success\"),\n",
    "        \"links\": simplified_links,\n",
    "        \"details\": launch.get(\"details\", \"\")\n",
    "    }\n",
    "\n",
    "def convert_row_to_dict(row):\n",
    "    \"\"\"Convert Spark Row to Python dict\"\"\"\n",
    "    def convert_value(value):\n",
    "        if hasattr(value, '__dict__') or isinstance(value, dict):\n",
    "            return dict(value)\n",
    "        elif isinstance(value, list):\n",
    "            return [convert_value(v) for v in value]\n",
    "        return value\n",
    "    \n",
    "    return {field.name: convert_value(getattr(row, field.name)) for field in row.__fields__}\n",
    "\n",
    "def main():\n",
    "    pipeline_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        spark = SparkSession.builder.appName(\"SpaceXDataPipeline\").getOrCreate()\n",
    "        \n",
    "        # 1. Fetch data\n",
    "        launches_data = fetch_data(\"https://api.spacexdata.com/v3/launches\", \"launches\")\n",
    "        rockets_data = fetch_data(\"https://api.spacexdata.com/v3/rockets\", \"rockets\")\n",
    "        \n",
    "        # 2. Process data\n",
    "        log_status(\"Starting data processing...\")\n",
    "        process_start = time.time()\n",
    "        \n",
    "        # Transform and create DataFrames\n",
    "        transformed_launches = [transform_launch_data(launch) for launch in launches_data]\n",
    "        launches_df = spark.createDataFrame(transformed_launches, schema=get_launches_schema())\n",
    "        rockets_df = spark.createDataFrame(rockets_data, schema=get_rockets_schema())\n",
    "        \n",
    "        # Join data\n",
    "        rockets_df = rockets_df.select(\n",
    "            col(\"rocket_id\"), \n",
    "            col(\"rocket_name\").alias(\"rocket_name_from_join\")\n",
    "        )\n",
    "        \n",
    "        combined_df = launches_df.join(\n",
    "            rockets_df, \n",
    "            launches_df.rocket.rocket_id == rockets_df.rocket_id,\n",
    "            \"left\"\n",
    "        )\n",
    "        \n",
    "        # Update nested field\n",
    "        combined_df = combined_df.withColumn(\n",
    "            \"rocket\",\n",
    "            col(\"rocket\").withField(\n",
    "                \"rocket_name\",\n",
    "                col(\"rocket_name_from_join\")\n",
    "            )\n",
    "        ).drop(\"rocket_name_from_join\", \"rocket_id\")\n",
    "        \n",
    "        # Convert to Python dicts\n",
    "        combined_data = [convert_row_to_dict(row) for row in combined_df.collect()]\n",
    "        \n",
    "        process_elapsed = time.time() - process_start\n",
    "        log_status(f\"Data processing completed in {process_elapsed:.2f} seconds\")\n",
    "        log_status(f\"Combined {len(combined_data)} launch records with rocket names\")\n",
    "        \n",
    "        # 3. Send data\n",
    "        send_success = send_data(\"https://httpbin.org/post\", combined_data)\n",
    "        \n",
    "        if send_success:\n",
    "            log_status(\"Pipeline completed successfully\")\n",
    "        else:\n",
    "            log_status(\"Pipeline completed with errors in sending step\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        log_status(f\"Pipeline failed with error: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        pipeline_elapsed = time.time() - pipeline_start\n",
    "        log_status(f\"Total pipeline execution time: {pipeline_elapsed:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6587bb33-e438-447d-ba82-89cd6dc699ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "try another\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b58b83dd-a8ca-4b58-a4a3-062589adee59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, MapType\n",
    "\n",
    "def log_status(message):\n",
    "    \"\"\"Helper function to log status messages with timestamp\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"[{timestamp}] {message}\")\n",
    "\n",
    "def fetch_data(url, description):\n",
    "    \"\"\"Fetch data from API with error handling\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        log_status(f\"Fetching {description} from {url}\")\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        elapsed = time.time() - start_time\n",
    "        log_status(f\"Successfully fetched {len(data) if isinstance(data, list) else 1} {description} records in {elapsed:.2f} seconds\")\n",
    "        return data\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        log_status(f\"Error fetching {description}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def send_data(url, data):\n",
    "    \"\"\"Send data to endpoint with error handling\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        log_status(f\"Sending combined data to {url}\")\n",
    "        \n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        response = requests.post(url, data=json.dumps(data), headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        log_status(f\"Data successfully sent in {elapsed:.2f} seconds\")\n",
    "        log_status(f\"Response status: {response.status_code}\")\n",
    "        log_status(f\"Response content: {response.text[:200]}...\")\n",
    "        return True\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        log_status(f\"Error sending data: {str(e)}\")\n",
    "        if hasattr(e, 'response') and e.response:\n",
    "            log_status(f\"Response content: {e.response.text[:200]}...\")\n",
    "        return False\n",
    "\n",
    "def get_launches_schema():\n",
    "    \"\"\"Define schema for launches data\"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"flight_number\", IntegerType(), True),\n",
    "        StructField(\"mission_name\", StringType(), True),\n",
    "        StructField(\"launch_year\", StringType(), True),\n",
    "        StructField(\"launch_date_utc\", StringType(), True),\n",
    "        StructField(\"rocket\", StructType([\n",
    "            StructField(\"rocket_id\", StringType(), True),\n",
    "            StructField(\"rocket_name\", StringType(), True),\n",
    "            StructField(\"rocket_type\", StringType(), True)\n",
    "        ]), True),\n",
    "        StructField(\"launch_success\", BooleanType(), True),\n",
    "        StructField(\"links\", MapType(StringType(), StringType()), True),\n",
    "        StructField(\"details\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "def get_rockets_schema():\n",
    "    \"\"\"Define schema for rockets data\"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"rocket_id\", StringType(), True),\n",
    "        StructField(\"rocket_name\", StringType(), True),\n",
    "        StructField(\"rocket_type\", StringType(), True),\n",
    "        StructField(\"active\", BooleanType(), True),\n",
    "        StructField(\"cost_per_launch\", IntegerType(), True),\n",
    "        StructField(\"success_rate_pct\", IntegerType(), True),\n",
    "        StructField(\"first_flight\", StringType(), True),\n",
    "        StructField(\"country\", StringType(), True),\n",
    "        StructField(\"company\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "def transform_launch_data(launch):\n",
    "    \"\"\"Transform launch data to match our schema\"\"\"\n",
    "    links = launch.get(\"links\", {})\n",
    "    simplified_links = {}\n",
    "    \n",
    "    for key, value in links.items():\n",
    "        if isinstance(value, dict):\n",
    "            simplified_links[key] = json.dumps(value)\n",
    "        elif value is None:\n",
    "            simplified_links[key] = \"\"\n",
    "        else:\n",
    "            simplified_links[key] = str(value)\n",
    "    \n",
    "    return {\n",
    "        \"flight_number\": launch.get(\"flight_number\"),\n",
    "        \"mission_name\": launch.get(\"mission_name\"),\n",
    "        \"launch_year\": launch.get(\"launch_year\"),\n",
    "        \"launch_date_utc\": launch.get(\"launch_date_utc\"),\n",
    "        \"rocket\": {\n",
    "            \"rocket_id\": launch.get(\"rocket\", {}).get(\"rocket_id\"),\n",
    "            \"rocket_name\": None,\n",
    "            \"rocket_type\": launch.get(\"rocket\", {}).get(\"rocket_type\")\n",
    "        },\n",
    "        \"launch_success\": launch.get(\"launch_success\"),\n",
    "        \"links\": simplified_links,\n",
    "        \"details\": launch.get(\"details\", \"\")\n",
    "    }\n",
    "\n",
    "def row_to_dict(row):\n",
    "    \"\"\"Convert Spark Row to Python dictionary\"\"\"\n",
    "    if hasattr(row, \"__fields__\"):\n",
    "        # For newer Spark versions\n",
    "        return {field.name: row_to_dict(getattr(row, field.name)) for field in row.__fields__}\n",
    "    elif isinstance(row, dict):\n",
    "        return {k: row_to_dict(v) for k, v in row.items()}\n",
    "    elif isinstance(row, (list, tuple)):\n",
    "        return [row_to_dict(v) for v in row]\n",
    "    else:\n",
    "        return row\n",
    "\n",
    "def main():\n",
    "    pipeline_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        spark = SparkSession.builder.appName(\"SpaceXDataPipeline\").getOrCreate()\n",
    "        \n",
    "        # 1. Fetch data\n",
    "        launches_data = fetch_data(\"https://api.spacexdata.com/v3/launches\", \"launches\")\n",
    "        rockets_data = fetch_data(\"https://api.spacexdata.com/v3/rockets\", \"rockets\")\n",
    "        \n",
    "        # 2. Process data\n",
    "        log_status(\"Starting data processing...\")\n",
    "        process_start = time.time()\n",
    "        \n",
    "        # Transform and create DataFrames\n",
    "        transformed_launches = [transform_launch_data(launch) for launch in launches_data]\n",
    "        launches_df = spark.createDataFrame(transformed_launches, schema=get_launches_schema())\n",
    "        rockets_df = spark.createDataFrame(rockets_data, schema=get_rockets_schema())\n",
    "        \n",
    "        # Join data\n",
    "        rockets_df = rockets_df.select(\n",
    "            col(\"rocket_id\"), \n",
    "            col(\"rocket_name\").alias(\"rocket_name_from_join\")\n",
    "        )\n",
    "        \n",
    "        combined_df = launches_df.join(\n",
    "            rockets_df, \n",
    "            launches_df.rocket.rocket_id == rockets_df.rocket_id,\n",
    "            \"left\"\n",
    "        )\n",
    "        \n",
    "        # Update nested field\n",
    "        combined_df = combined_df.withColumn(\n",
    "            \"rocket\",\n",
    "            col(\"rocket\").withField(\n",
    "                \"rocket_name\",\n",
    "                col(\"rocket_name_from_join\")\n",
    "            )\n",
    "        ).drop(\"rocket_name_from_join\", \"rocket_id\")\n",
    "        \n",
    "        # Convert to Python dicts\n",
    "        combined_data = [row_to_dict(row) for row in combined_df.collect()]\n",
    "        \n",
    "        process_elapsed = time.time() - process_start\n",
    "        log_status(f\"Data processing completed in {process_elapsed:.2f} seconds\")\n",
    "        log_status(f\"Combined {len(combined_data)} launch records with rocket names\")\n",
    "        \n",
    "        # 3. Send data\n",
    "        send_success = send_data(\"https://httpbin.org/post\", combined_data)\n",
    "        \n",
    "        if send_success:\n",
    "            log_status(\"Pipeline completed successfully\")\n",
    "        else:\n",
    "            log_status(\"Pipeline completed with errors in sending step\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        log_status(f\"Pipeline failed with error: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        pipeline_elapsed = time.time() - pipeline_start\n",
    "        log_status(f\"Total pipeline execution time: {pipeline_elapsed:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dea2b28-a121-4fee-8919-050b43a3d8f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "try again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2901982-7861-42c1-960e-f6eaa5c33944",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType, MapType\n",
    "\n",
    "def log_status(message):\n",
    "    \"\"\"Helper function to log status messages with timestamp\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"[{timestamp}] {message}\")\n",
    "\n",
    "def fetch_data(url, description):\n",
    "    \"\"\"Fetch data from API with error handling\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        log_status(f\"Fetching {description} from {url}\")\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        data = response.json()\n",
    "        elapsed = time.time() - start_time\n",
    "        log_status(f\"Successfully fetched {len(data) if isinstance(data, list) else 1} {description} records in {elapsed:.2f} seconds\")\n",
    "        return data\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        log_status(f\"Error fetching {description}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def send_data(url, data):\n",
    "    \"\"\"Send data to endpoint with error handling\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        log_status(f\"Sending combined data to {url}\")\n",
    "        \n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        response = requests.post(url, data=json.dumps(data), headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        log_status(f\"Data successfully sent in {elapsed:.2f} seconds\")\n",
    "        log_status(f\"Response status: {response.status_code}\")\n",
    "        log_status(f\"Response content: {response.text[:200]}...\")\n",
    "        return True\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        log_status(f\"Error sending data: {str(e)}\")\n",
    "        if hasattr(e, 'response') and e.response:\n",
    "            log_status(f\"Response content: {e.response.text[:200]}...\")\n",
    "        return False\n",
    "\n",
    "def get_launches_schema():\n",
    "    \"\"\"Define schema for launches data\"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"flight_number\", IntegerType(), True),\n",
    "        StructField(\"mission_name\", StringType(), True),\n",
    "        StructField(\"launch_year\", StringType(), True),\n",
    "        StructField(\"launch_date_utc\", StringType(), True),\n",
    "        StructField(\"rocket\", StructType([\n",
    "            StructField(\"rocket_id\", StringType(), True),\n",
    "            StructField(\"rocket_name\", StringType(), True),\n",
    "            StructField(\"rocket_type\", StringType(), True)\n",
    "        ]), True),\n",
    "        StructField(\"launch_success\", BooleanType(), True),\n",
    "        StructField(\"links\", MapType(StringType(), StringType()), True),\n",
    "        StructField(\"details\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "def get_rockets_schema():\n",
    "    \"\"\"Define schema for rockets data\"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"rocket_id\", StringType(), True),\n",
    "        StructField(\"rocket_name\", StringType(), True),\n",
    "        StructField(\"rocket_type\", StringType(), True),\n",
    "        StructField(\"active\", BooleanType(), True),\n",
    "        StructField(\"cost_per_launch\", IntegerType(), True),\n",
    "        StructField(\"success_rate_pct\", IntegerType(), True),\n",
    "        StructField(\"first_flight\", StringType(), True),\n",
    "        StructField(\"country\", StringType(), True),\n",
    "        StructField(\"company\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "def transform_launch_data(launch):\n",
    "    \"\"\"Transform launch data to match our schema\"\"\"\n",
    "    links = launch.get(\"links\", {})\n",
    "    simplified_links = {}\n",
    "    \n",
    "    for key, value in links.items():\n",
    "        if isinstance(value, dict):\n",
    "            simplified_links[key] = json.dumps(value)\n",
    "        elif value is None:\n",
    "            simplified_links[key] = \"\"\n",
    "        else:\n",
    "            simplified_links[key] = str(value)\n",
    "    \n",
    "    return {\n",
    "        \"flight_number\": launch.get(\"flight_number\"),\n",
    "        \"mission_name\": launch.get(\"mission_name\"),\n",
    "        \"launch_year\": launch.get(\"launch_year\"),\n",
    "        \"launch_date_utc\": launch.get(\"launch_date_utc\"),\n",
    "        \"rocket\": {\n",
    "            \"rocket_id\": launch.get(\"rocket\", {}).get(\"rocket_id\"),\n",
    "            \"rocket_name\": None,\n",
    "            \"rocket_type\": launch.get(\"rocket\", {}).get(\"rocket_type\")\n",
    "        },\n",
    "        \"launch_success\": launch.get(\"launch_success\"),\n",
    "        \"links\": simplified_links,\n",
    "        \"details\": launch.get(\"details\", \"\")\n",
    "    }\n",
    "\n",
    "def row_to_dict(row):\n",
    "    \"\"\"Convert Spark Row to Python dictionary - Universal version\"\"\"\n",
    "    if row is None:\n",
    "        return None\n",
    "    elif isinstance(row, (str, int, float, bool)):\n",
    "        return row\n",
    "    elif hasattr(row, \"asDict\"):  # For standard Spark Rows\n",
    "        return {k: row_to_dict(v) for k, v in row.asDict().items()}\n",
    "    elif isinstance(row, dict):\n",
    "        return {k: row_to_dict(v) for k, v in row.items()}\n",
    "    elif isinstance(row, (list, tuple)):\n",
    "        return [row_to_dict(v) for v in row]\n",
    "    else:\n",
    "        return str(row)  # Fallback for any other type\n",
    "\n",
    "def main():\n",
    "    pipeline_start = time.time()\n",
    "    \n",
    "    try:\n",
    "        spark = SparkSession.builder.appName(\"SpaceXDataPipeline\").getOrCreate()\n",
    "        \n",
    "        # 1. Fetch data\n",
    "        launches_data = fetch_data(\"https://api.spacexdata.com/v3/launches\", \"launches\")\n",
    "        rockets_data = fetch_data(\"https://api.spacexdata.com/v3/rockets\", \"rockets\")\n",
    "        \n",
    "        # 2. Process data\n",
    "        log_status(\"Starting data processing...\")\n",
    "        process_start = time.time()\n",
    "        \n",
    "        # Transform and create DataFrames\n",
    "        transformed_launches = [transform_launch_data(launch) for launch in launches_data]\n",
    "        launches_df = spark.createDataFrame(transformed_launches, schema=get_launches_schema())\n",
    "        rockets_df = spark.createDataFrame(rockets_data, schema=get_rockets_schema())\n",
    "        \n",
    "        # Join data\n",
    "        rockets_df = rockets_df.select(\n",
    "            col(\"rocket_id\"), \n",
    "            col(\"rocket_name\").alias(\"rocket_name_from_join\")\n",
    "        )\n",
    "        \n",
    "        combined_df = launches_df.join(\n",
    "            rockets_df, \n",
    "            launches_df.rocket.rocket_id == rockets_df.rocket_id,\n",
    "            \"left\"\n",
    "        )\n",
    "        \n",
    "        # Update nested field\n",
    "        combined_df = combined_df.withColumn(\n",
    "            \"rocket\",\n",
    "            col(\"rocket\").withField(\n",
    "                \"rocket_name\",\n",
    "                col(\"rocket_name_from_join\")\n",
    "            )\n",
    "        ).drop(\"rocket_name_from_join\", \"rocket_id\")\n",
    "        \n",
    "        # Convert to Python dicts using our universal converter\n",
    "        combined_data = [row_to_dict(row) for row in combined_df.collect()]\n",
    "        \n",
    "        process_elapsed = time.time() - process_start\n",
    "        log_status(f\"Data processing completed in {process_elapsed:.2f} seconds\")\n",
    "        log_status(f\"Combined {len(combined_data)} launch records with rocket names\")\n",
    "        \n",
    "        # 3. Send data\n",
    "        send_success = send_data(\"https://httpbin.org/post\", combined_data)\n",
    "        \n",
    "        if send_success:\n",
    "            log_status(\"Pipeline completed successfully\")\n",
    "        else:\n",
    "            log_status(\"Pipeline completed with errors in sending step\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        log_status(f\"Pipeline failed with error: {str(e)}\")\n",
    "        raise\n",
    "    finally:\n",
    "        pipeline_elapsed = time.time() - pipeline_start\n",
    "        log_status(f\"Total pipeline execution time: {pipeline_elapsed:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "152b2938-41c1-4517-89d9-54585c1cc4ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Merge DeepSeek-V3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
