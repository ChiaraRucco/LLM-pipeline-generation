{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6013620-52de-4e21-aa7c-ea5c0d9a24a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You are an expert Python developer specializing in the Databricks environment. Your task is to create a complete Python script to be executed within a Databricks notebook. The script must perform the following operations:\n",
    "1.\tData Retrieval from SpaceX API:\n",
    "o\tInteract with the SpaceX v3 REST API (https://api.spacexdata.com/v3).\n",
    "o\tRetrieve data from one specific endpoint likely containing numerical data where missing values might occur: \n",
    "\tAll launches: https://api.spacexdata.com/v3/launches\n",
    "\t(Self-correction: While launches is common, /cores might be a better example for potential missing numericals like reuse_count, rtls_landings etc. Let's use /cores for a potentially more illustrative example, but keep /launches as an alternative)\n",
    "\tAlternative/Primary: All Cores: https://api.spacexdata.com/v3/cores\n",
    "o\tHandle potential errors during the API calls (e.g., timeouts, non-200 status codes).\n",
    "2.\tMissing Value Imputation (Mean):\n",
    "o\tPerform mean imputation on the retrieved data (list of dictionaries).\n",
    "o\tImputation Logic: \n",
    "\tIdentify Numerical Fields: First, automatically identify the keys/fields within the dictionaries that predominantly contain numerical values (int or float). You might need to inspect the first few records or a sample to determine these fields reliably, or iterate through all records checking types.\n",
    "\tCalculate Mean per Field: For each identified numerical field, calculate the mean using only the existing, non-missing (not None) numerical values across all records in the dataset.\n",
    "\tImpute Missing Values: Iterate through the dataset again. For each numerical field, replace any missing values (represented as None) with the pre-calculated mean for that specific field.\n",
    "\tHandle Edge Cases: If a numerical field contains only missing values (or no valid numbers to calculate a mean), log a warning and leave the missing values as None (or impute with 0, please specify preference - let's default to leaving them None and logging).\n",
    "o\tThe final result should be the original list of dictionaries, but with missing numerical values replaced by the calculated mean for their respective fields.\n",
    "3.\tControl Parameters and Debugging:\n",
    "o\tInclude a variable at the beginning of the script to define the API endpoint URL, making it easily modifiable: \n",
    "\tAPI_ENDPOINT_URL = \"https://api.spacexdata.com/v3/cores\" #(or /launches)\n",
    "o\tUse Python's standard logging module to provide informative output during execution. Configure logging to display messages at the INFO level.\n",
    "o\tLog key messages such as: starting data retrieval, number of records retrieved, starting imputation process, identified numerical fields potentially needing imputation (e.g., ['reuse_count', 'rtls_attempts', ...]), calculated mean for field X, number of missing values imputed for field X, any warnings for fields with no calculable mean, imputation complete, starting upload to httpbin, upload outcome.\n",
    "4.\tExecution Time Measurement:\n",
    "o\tCode Execution Time: Measure the time taken to perform the main operations (data retrieval + imputation). Print this time after the imputation operation is complete.\n",
    "o\tPipeline Execution Time: Measure the total execution time of the entire script (from the beginning until after the upload to httpbin). Print this total time at the end of the script. Use Python's time module.\n",
    "5.\tUpload Result:\n",
    "o\tTake the resulting imputed list of dictionaries from the imputation operation.\n",
    "o\tSerialize it into JSON format.\n",
    "o\tMake an HTTP POST request to the https://httpbin.org/post endpoint, sending the resulting imputed JSON data in the request body.\n",
    "o\tVerify the response from httpbin.org (e.g., check the status code) and log the outcome of the upload operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acadff3a-4609-4a37-a316-cd5989c71be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SpaceX API Data Processing with Mean Imputation\n",
    "# For execution in Databricks notebooks\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import statistics\n",
    "from typing import Dict, List, Any, Union, Optional\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# API Configuration\n",
    "API_ENDPOINT_URL = \"https://api.spacexdata.com/v3/cores\"  # Can be modified to /launches if needed\n",
    "HTTPBIN_URL = \"https://httpbin.org/post\"\n",
    "REQUEST_TIMEOUT = 30  # seconds\n",
    "\n",
    "def retrieve_spacex_data() -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Retrieves data from the SpaceX API.\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing SpaceX data.\n",
    "    \n",
    "    Raises:\n",
    "        Exception: If the API request fails.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting data retrieval from {API_ENDPOINT_URL}\")\n",
    "    try:\n",
    "        response = requests.get(API_ENDPOINT_URL, timeout=REQUEST_TIMEOUT)\n",
    "        response.raise_for_status()  # Raises an exception for 4XX/5XX responses\n",
    "        data = response.json()\n",
    "        logger.info(f\"Successfully retrieved {len(data)} records from the SpaceX API\")\n",
    "        return data\n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.error(f\"Request to {API_ENDPOINT_URL} timed out after {REQUEST_TIMEOUT} seconds\")\n",
    "        raise\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        logger.error(f\"HTTP error occurred: {e}\")\n",
    "        raise\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Error during request to SpaceX API: {e}\")\n",
    "        raise\n",
    "    except json.JSONDecodeError:\n",
    "        logger.error(\"Failed to decode JSON response from SpaceX API\")\n",
    "        raise\n",
    "\n",
    "def identify_numerical_fields(data: List[Dict[str, Any]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Identifies keys in the data that predominantly contain numerical values.\n",
    "    \n",
    "    Args:\n",
    "        data: List of dictionaries containing SpaceX data.\n",
    "        \n",
    "    Returns:\n",
    "        List of keys that contain numerical values.\n",
    "    \"\"\"\n",
    "    logger.info(\"Identifying numerical fields in the dataset\")\n",
    "    \n",
    "    if not data:\n",
    "        logger.warning(\"Empty dataset provided, cannot identify numerical fields\")\n",
    "        return []\n",
    "    \n",
    "    # Track counts of numerical values for each field\n",
    "    field_counts = {}\n",
    "    \n",
    "    # Examine each record in the dataset\n",
    "    for record in data:\n",
    "        for key, value in record.items():\n",
    "            if key not in field_counts:\n",
    "                field_counts[key] = {\"numerical\": 0, \"total\": 0}\n",
    "            \n",
    "            field_counts[key][\"total\"] += 1\n",
    "            \n",
    "            # Check if the value is a number (int or float) but not a boolean\n",
    "            if isinstance(value, (int, float)) and not isinstance(value, bool):\n",
    "                field_counts[key][\"numerical\"] += 1\n",
    "    \n",
    "    # Consider a field numerical if at least 50% of non-None values are numbers\n",
    "    numerical_fields = []\n",
    "    for key, counts in field_counts.items():\n",
    "        if counts[\"total\"] > 0 and counts[\"numerical\"] / counts[\"total\"] >= 0.5:\n",
    "            numerical_fields.append(key)\n",
    "    \n",
    "    logger.info(f\"Identified numerical fields: {numerical_fields}\")\n",
    "    return numerical_fields\n",
    "\n",
    "def calculate_means(data: List[Dict[str, Any]], numerical_fields: List[str]) -> Dict[str, Optional[float]]:\n",
    "    \"\"\"\n",
    "    Calculates the mean for each numerical field in the data.\n",
    "    \n",
    "    Args:\n",
    "        data: List of dictionaries containing SpaceX data.\n",
    "        numerical_fields: List of keys that contain numerical values.\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping field names to their mean values.\n",
    "    \"\"\"\n",
    "    logger.info(\"Calculating means for numerical fields\")\n",
    "    means = {}\n",
    "    \n",
    "    for field in numerical_fields:\n",
    "        # Collect all non-None numerical values for this field\n",
    "        values = [\n",
    "            record[field] for record in data \n",
    "            if field in record and \n",
    "            record[field] is not None and \n",
    "            isinstance(record[field], (int, float)) and\n",
    "            not isinstance(record[field], bool)\n",
    "        ]\n",
    "        \n",
    "        # Calculate mean if there are values\n",
    "        if values:\n",
    "            field_mean = statistics.mean(values)\n",
    "            means[field] = field_mean\n",
    "            logger.info(f\"Calculated mean for field '{field}': {field_mean}\")\n",
    "        else:\n",
    "            means[field] = None\n",
    "            logger.warning(f\"Field '{field}' has no valid numerical values to calculate mean\")\n",
    "    \n",
    "    return means\n",
    "\n",
    "def perform_mean_imputation(data: List[Dict[str, Any]], numerical_fields: List[str], means: Dict[str, Optional[float]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Imputes missing values in numerical fields with their respective means.\n",
    "    \n",
    "    Args:\n",
    "        data: List of dictionaries containing SpaceX data.\n",
    "        numerical_fields: List of keys that contain numerical values.\n",
    "        means: Dictionary mapping field names to their mean values.\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries with imputed values.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting mean imputation process\")\n",
    "    imputed_data = []\n",
    "    imputation_counts = {field: 0 for field in numerical_fields}\n",
    "    \n",
    "    for record in data:\n",
    "        imputed_record = record.copy()\n",
    "        \n",
    "        for field in numerical_fields:\n",
    "            # Skip fields not present in this record\n",
    "            if field not in imputed_record:\n",
    "                continue\n",
    "            \n",
    "            # Skip fields where we couldn't calculate a mean\n",
    "            if means[field] is None:\n",
    "                continue\n",
    "                \n",
    "            # Impute missing values\n",
    "            if imputed_record[field] is None:\n",
    "                imputed_record[field] = means[field]\n",
    "                imputation_counts[field] += 1\n",
    "        \n",
    "        imputed_data.append(imputed_record)\n",
    "    \n",
    "    # Log imputation counts\n",
    "    for field, count in imputation_counts.items():\n",
    "        if count > 0:\n",
    "            logger.info(f\"Imputed {count} missing values for field '{field}'\")\n",
    "        else:\n",
    "            logger.info(f\"No imputation needed for field '{field}'\")\n",
    "    \n",
    "    logger.info(\"Imputation process complete\")\n",
    "    return imputed_data\n",
    "\n",
    "def upload_to_httpbin(data: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Uploads the imputed data to httpbin.org.\n",
    "    \n",
    "    Args:\n",
    "        data: List of dictionaries with imputed values.\n",
    "        \n",
    "    Returns:\n",
    "        Response from httpbin.org.\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If the upload fails.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting upload to {HTTPBIN_URL}\")\n",
    "    try:\n",
    "        # Serialize the data to JSON\n",
    "        json_data = json.dumps(data)\n",
    "        \n",
    "        # Upload to httpbin.org\n",
    "        response = requests.post(\n",
    "            HTTPBIN_URL, \n",
    "            data=json_data,\n",
    "            headers={\"Content-Type\": \"application/json\"},\n",
    "            timeout=REQUEST_TIMEOUT\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse the response\n",
    "        result = response.json()\n",
    "        logger.info(f\"Successfully uploaded data to {HTTPBIN_URL}\")\n",
    "        return result\n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.error(f\"Request to {HTTPBIN_URL} timed out after {REQUEST_TIMEOUT} seconds\")\n",
    "        raise\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        logger.error(f\"HTTP error occurred during upload: {e}\")\n",
    "        raise\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Error during upload to httpbin: {e}\")\n",
    "        raise\n",
    "    except json.JSONDecodeError:\n",
    "        logger.error(\"Failed to decode JSON response from httpbin\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function that executes the entire data processing pipeline.\n",
    "    \"\"\"\n",
    "    # Start timing the whole pipeline\n",
    "    pipeline_start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Start timing the data retrieval and imputation\n",
    "        processing_start_time = time.time()\n",
    "        \n",
    "        # Step 1: Retrieve data from SpaceX API\n",
    "        data = retrieve_spacex_data()\n",
    "        \n",
    "        # Step 2: Identify numerical fields\n",
    "        numerical_fields = identify_numerical_fields(data)\n",
    "        \n",
    "        # Step 3: Calculate means for numerical fields\n",
    "        means = calculate_means(data, numerical_fields)\n",
    "        \n",
    "        # Step 4: Perform mean imputation\n",
    "        imputed_data = perform_mean_imputation(data, numerical_fields, means)\n",
    "        \n",
    "        # Calculate and log processing time\n",
    "        processing_time = time.time() - processing_start_time\n",
    "        logger.info(f\"Data retrieval and imputation completed in {processing_time:.2f} seconds\")\n",
    "        \n",
    "        # Step 5: Upload the imputed data to httpbin.org\n",
    "        upload_result = upload_to_httpbin(imputed_data)\n",
    "        logger.info(\"Upload result received from httpbin\")\n",
    "        \n",
    "        # Calculate and log total pipeline time\n",
    "        pipeline_time = time.time() - pipeline_start_time\n",
    "        logger.info(f\"Total pipeline execution completed in {pipeline_time:.2f} seconds\")\n",
    "        \n",
    "        # Return the imputed data for further use in the notebook if needed\n",
    "        return imputed_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline execution failed: {e}\")\n",
    "        # Calculate time even in case of failure\n",
    "        pipeline_time = time.time() - pipeline_start_time\n",
    "        logger.info(f\"Pipeline execution failed after {pipeline_time:.2f} seconds\")\n",
    "        raise\n",
    "\n",
    "# Execute the main function when the notebook cell is run\n",
    "if __name__ == \"__main__\":\n",
    "    imputed_data = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6c2d32e-121d-424b-ab13-45e41c13e8b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CLAUDE 3.7 SONNET",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
