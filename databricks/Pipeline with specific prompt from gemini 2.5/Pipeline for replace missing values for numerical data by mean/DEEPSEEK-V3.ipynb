{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6013620-52de-4e21-aa7c-ea5c0d9a24a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You are an expert Python developer specializing in the Databricks environment. Your task is to create a complete Python script to be executed within a Databricks notebook. The script must perform the following operations:\n",
    "1.\tData Retrieval from SpaceX API:\n",
    "o\tInteract with the SpaceX v3 REST API (https://api.spacexdata.com/v3).\n",
    "o\tRetrieve data from one specific endpoint likely containing numerical data where missing values might occur: \n",
    "\tAll launches: https://api.spacexdata.com/v3/launches\n",
    "\t(Self-correction: While launches is common, /cores might be a better example for potential missing numericals like reuse_count, rtls_landings etc. Let's use /cores for a potentially more illustrative example, but keep /launches as an alternative)\n",
    "\tAlternative/Primary: All Cores: https://api.spacexdata.com/v3/cores\n",
    "o\tHandle potential errors during the API calls (e.g., timeouts, non-200 status codes).\n",
    "2.\tMissing Value Imputation (Mean):\n",
    "o\tPerform mean imputation on the retrieved data (list of dictionaries).\n",
    "o\tImputation Logic: \n",
    "\tIdentify Numerical Fields: First, automatically identify the keys/fields within the dictionaries that predominantly contain numerical values (int or float). You might need to inspect the first few records or a sample to determine these fields reliably, or iterate through all records checking types.\n",
    "\tCalculate Mean per Field: For each identified numerical field, calculate the mean using only the existing, non-missing (not None) numerical values across all records in the dataset.\n",
    "\tImpute Missing Values: Iterate through the dataset again. For each numerical field, replace any missing values (represented as None) with the pre-calculated mean for that specific field.\n",
    "\tHandle Edge Cases: If a numerical field contains only missing values (or no valid numbers to calculate a mean), log a warning and leave the missing values as None (or impute with 0, please specify preference - let's default to leaving them None and logging).\n",
    "o\tThe final result should be the original list of dictionaries, but with missing numerical values replaced by the calculated mean for their respective fields.\n",
    "3.\tControl Parameters and Debugging:\n",
    "o\tInclude a variable at the beginning of the script to define the API endpoint URL, making it easily modifiable: \n",
    "\tAPI_ENDPOINT_URL = \"https://api.spacexdata.com/v3/cores\" #(or /launches)\n",
    "o\tUse Python's standard logging module to provide informative output during execution. Configure logging to display messages at the INFO level.\n",
    "o\tLog key messages such as: starting data retrieval, number of records retrieved, starting imputation process, identified numerical fields potentially needing imputation (e.g., ['reuse_count', 'rtls_attempts', ...]), calculated mean for field X, number of missing values imputed for field X, any warnings for fields with no calculable mean, imputation complete, starting upload to httpbin, upload outcome.\n",
    "4.\tExecution Time Measurement:\n",
    "o\tCode Execution Time: Measure the time taken to perform the main operations (data retrieval + imputation). Print this time after the imputation operation is complete.\n",
    "o\tPipeline Execution Time: Measure the total execution time of the entire script (from the beginning until after the upload to httpbin). Print this total time at the end of the script. Use Python's time module.\n",
    "5.\tUpload Result:\n",
    "o\tTake the resulting imputed list of dictionaries from the imputation operation.\n",
    "o\tSerialize it into JSON format.\n",
    "o\tMake an HTTP POST request to the https://httpbin.org/post endpoint, sending the resulting imputed JSON data in the request body.\n",
    "o\tVerify the response from httpbin.org (e.g., check the status code) and log the outcome of the upload operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acadff3a-4609-4a37-a316-cd5989c71be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import logging\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Control parameters\n",
    "API_ENDPOINT_URL = \"https://api.spacexdata.com/v3/cores\"  # or \"https://api.spacexdata.com/v3/launches\"\n",
    "HTTPBIN_UPLOAD_URL = \"https://httpbin.org/post\"\n",
    "\n",
    "def fetch_data_from_api(url: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Fetch data from the specified SpaceX API endpoint.\"\"\"\n",
    "    logger.info(f\"Starting data retrieval from {url}\")\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()  # Raises an HTTPError for bad responses (4xx, 5xx)\n",
    "        data = response.json()\n",
    "        logger.info(f\"Successfully retrieved {len(data)} records\")\n",
    "        return data\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Error fetching data from API: {e}\")\n",
    "        raise\n",
    "\n",
    "def identify_numerical_fields(data: List[Dict[str, Any]]) -> List[str]:\n",
    "    \"\"\"Identify fields in the data that predominantly contain numerical values.\"\"\"\n",
    "    numerical_fields = []\n",
    "    if not data:\n",
    "        return numerical_fields\n",
    "    \n",
    "    # Sample the first few records to identify numerical fields\n",
    "    sample_records = data[:10]\n",
    "    field_types = {}\n",
    "    \n",
    "    for record in sample_records:\n",
    "        for key, value in record.items():\n",
    "            if value is None:\n",
    "                continue\n",
    "            field_type = type(value)\n",
    "            if field_type in (int, float):\n",
    "                if key not in field_types:\n",
    "                    field_types[key] = 0\n",
    "                field_types[key] += 1\n",
    "    \n",
    "    # Consider a field numerical if it appears as int/float in at least half the sampled records\n",
    "    threshold = len(sample_records) / 2\n",
    "    numerical_fields = [key for key, count in field_types.items() if count >= threshold]\n",
    "    \n",
    "    logger.info(f\"Identified numerical fields: {numerical_fields}\")\n",
    "    return numerical_fields\n",
    "\n",
    "def calculate_mean_for_fields(data: List[Dict[str, Any]], numerical_fields: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"Calculate the mean for each numerical field using non-missing values.\"\"\"\n",
    "    field_sums = {field: 0.0 for field in numerical_fields}\n",
    "    field_counts = {field: 0 for field in numerical_fields}\n",
    "    \n",
    "    for record in data:\n",
    "        for field in numerical_fields:\n",
    "            value = record.get(field)\n",
    "            if isinstance(value, (int, float)):\n",
    "                field_sums[field] += value\n",
    "                field_counts[field] += 1\n",
    "    \n",
    "    field_means = {}\n",
    "    for field in numerical_fields:\n",
    "        if field_counts[field] > 0:\n",
    "            field_means[field] = field_sums[field] / field_counts[field]\n",
    "        else:\n",
    "            field_means[field] = None\n",
    "            logger.warning(f\"Cannot calculate mean for field '{field}': no valid numerical values found\")\n",
    "    \n",
    "    for field, mean in field_means.items():\n",
    "        if mean is not None:\n",
    "            logger.info(f\"Calculated mean for field '{field}': {mean:.2f}\")\n",
    "    \n",
    "    return field_means\n",
    "\n",
    "def perform_mean_imputation(data: List[Dict[str, Any]], numerical_fields: List[str], field_means: Dict[str, float]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Perform mean imputation on the data for the specified numerical fields.\"\"\"\n",
    "    imputed_data = []\n",
    "    imputation_counts = {field: 0 for field in numerical_fields}\n",
    "    \n",
    "    for record in data:\n",
    "        imputed_record = record.copy()\n",
    "        for field in numerical_fields:\n",
    "            if field in imputed_record and imputed_record[field] is None and field_means[field] is not None:\n",
    "                imputed_record[field] = field_means[field]\n",
    "                imputation_counts[field] += 1\n",
    "        imputed_data.append(imputed_record)\n",
    "    \n",
    "    for field, count in imputation_counts.items():\n",
    "        if count > 0:\n",
    "            logger.info(f\"Imputed {count} missing values for field '{field}'\")\n",
    "    \n",
    "    return imputed_data\n",
    "\n",
    "def upload_to_httpbin(data: List[Dict[str, Any]], url: str) -> bool:\n",
    "    \"\"\"Upload the imputed data to httpbin.org and verify the response.\"\"\"\n",
    "    logger.info(\"Starting upload to httpbin.org\")\n",
    "    try:\n",
    "        response = requests.post(url, json=data, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        logger.info(\"Upload to httpbin.org successful\")\n",
    "        logger.debug(f\"Response from httpbin: {response.json()}\")\n",
    "        return True\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Error uploading data to httpbin: {e}\")\n",
    "        return False\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main pipeline execution.\"\"\"\n",
    "    pipeline_start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Data Retrieval\n",
    "        retrieval_start_time = time.time()\n",
    "        data = fetch_data_from_api(API_ENDPOINT_URL)\n",
    "        retrieval_time = time.time() - retrieval_start_time\n",
    "        logger.info(f\"Data retrieval completed in {retrieval_time:.2f} seconds\")\n",
    "        \n",
    "        # Step 2: Missing Value Imputation\n",
    "        imputation_start_time = time.time()\n",
    "        numerical_fields = identify_numerical_fields(data)\n",
    "        if numerical_fields:\n",
    "            field_means = calculate_mean_for_fields(data, numerical_fields)\n",
    "            imputed_data = perform_mean_imputation(data, numerical_fields, field_means)\n",
    "        else:\n",
    "            imputed_data = data\n",
    "            logger.info(\"No numerical fields identified for imputation\")\n",
    "        imputation_time = time.time() - imputation_start_time\n",
    "        logger.info(f\"Imputation completed in {imputation_time:.2f} seconds\")\n",
    "        \n",
    "        # Step 3: Upload Result\n",
    "        upload_start_time = time.time()\n",
    "        upload_success = upload_to_httpbin(imputed_data, HTTPBIN_UPLOAD_URL)\n",
    "        upload_time = time.time() - upload_start_time\n",
    "        logger.info(f\"Upload completed in {upload_time:.2f} seconds\")\n",
    "        \n",
    "        # Log total pipeline execution time\n",
    "        pipeline_time = time.time() - pipeline_start_time\n",
    "        logger.info(f\"Total pipeline execution time: {pipeline_time:.2f} seconds\")\n",
    "        \n",
    "        return upload_success\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline failed with error: {e}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8e20233-f174-4ba7-8391-c7cee400d871",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acefd7a9-5714-4ea5-b7e0-ce659c520a3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72cdddb2-10fe-4846-a456-65c4e1378342",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "DEEPSEEK-V3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
