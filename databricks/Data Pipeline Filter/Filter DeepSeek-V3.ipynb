{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1137619-5355-4fef-890f-52fe0a9dd7a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## PROMPT\n",
    "Generate a pipeline for Databricks:\n",
    "* Get Data information from two web addresses:\n",
    "   * Space launches: https://api.spacexdata.com/v3/launches\n",
    "* Filter the list of launches based on launch year and launch success status.\n",
    "* Send the chosen information to the web address: https://httpbin.org/post\n",
    "* The script must provide status updates on its progress, report any errors encountered, confirm the outcome of the final data sending step, and measure/report execution times.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ca76e07-1c35-41ac-9ca4-be469e51d881",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def log_step(message):\n",
    "    \"\"\"Helper function to log pipeline steps with timestamps\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"[{timestamp}] {message}\")\n",
    "\n",
    "def measure_time(func):\n",
    "    \"\"\"Decorator to measure execution time of functions\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        log_step(f\"Starting: {func.__name__}\")\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        log_step(f\"Completed: {func.__name__} in {end_time - start_time:.2f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@measure_time\n",
    "def fetch_spacex_launches():\n",
    "    \"\"\"Fetch SpaceX launches data from API\"\"\"\n",
    "    url = \"https://api.spacexdata.com/v3/launches\"\n",
    "    log_step(f\"Fetching data from {url}\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raises exception for 4XX/5XX errors\n",
    "        data = response.json()\n",
    "        log_step(f\"Successfully fetched {len(data)} launches\")\n",
    "        return data\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        log_step(f\"Error fetching SpaceX data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "@measure_time\n",
    "def filter_launches(launches, year=None, success_status=None):\n",
    "    \"\"\"Filter launches by year and success status\"\"\"\n",
    "    log_step(f\"Filtering {len(launches)} launches (year: {year}, success: {success_status})\")\n",
    "    \n",
    "    try:\n",
    "        # Convert to Spark DataFrame for efficient filtering\n",
    "        df = spark.createDataFrame(launches)\n",
    "        \n",
    "        # Apply filters if provided\n",
    "        if year is not None:\n",
    "            df = df.filter(col(\"launch_year\") == str(year))\n",
    "        if success_status is not None:\n",
    "            df = df.filter(col(\"launch_success\") == success_status)\n",
    "            \n",
    "        filtered_data = [row.asDict() for row in df.collect()]\n",
    "        log_step(f\"Found {len(filtered_data)} matching launches after filtering\")\n",
    "        return filtered_data\n",
    "    except Exception as e:\n",
    "        log_step(f\"Error filtering launches: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "@measure_time\n",
    "def send_to_endpoint(data, endpoint_url):\n",
    "    \"\"\"Send filtered data to specified endpoint\"\"\"\n",
    "    log_step(f\"Sending {len(data)} records to {endpoint_url}\")\n",
    "    \n",
    "    try:\n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        response = requests.post(endpoint_url, data=json.dumps(data), headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        log_step(f\"Successfully sent data. Response status: {response.status_code}\")\n",
    "        log_step(f\"Response content: {response.text[:200]}...\")  # Log first 200 chars of response\n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        log_step(f\"Error sending data to endpoint: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main pipeline execution\"\"\"\n",
    "    try:\n",
    "        # Configuration parameters\n",
    "        launch_year = 2020  # Set to None to disable year filter\n",
    "        success_status = True  # Set to None to disable success filter\n",
    "        endpoint_url = \"https://httpbin.org/post\"\n",
    "        \n",
    "        log_step(\"Starting SpaceX Data Pipeline\")\n",
    "        \n",
    "        # Step 1: Fetch data\n",
    "        launches = fetch_spacex_launches()\n",
    "        \n",
    "        # Step 2: Filter data\n",
    "        filtered_launches = filter_launches(launches, year=launch_year, success_status=success_status)\n",
    "        \n",
    "        # Step 3: Send data\n",
    "        send_response = send_to_endpoint(filtered_launches, endpoint_url)\n",
    "        \n",
    "        log_step(\"Pipeline completed successfully!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_step(f\"Pipeline failed with error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Execute the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    pipeline_success = main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f96509e-a9c7-4c2c-a54f-9ce4eed27b93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1 error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81cbaade-4997-4826-ae7f-89f9edd01961",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType, IntegerType, MapType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def log_step(message):\n",
    "    \"\"\"Helper function to log pipeline steps with timestamps\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"[{timestamp}] {message}\")\n",
    "\n",
    "def measure_time(func):\n",
    "    \"\"\"Decorator to measure execution time of functions\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        log_step(f\"Starting: {func.__name__}\")\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        log_step(f\"Completed: {func.__name__} in {end_time - start_time:.2f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@measure_time\n",
    "def fetch_spacex_launches():\n",
    "    \"\"\"Fetch SpaceX launches data from API\"\"\"\n",
    "    url = \"https://api.spacexdata.com/v3/launches\"\n",
    "    log_step(f\"Fetching data from {url}\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        log_step(f\"Successfully fetched {len(data)} launches\")\n",
    "        return data\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        log_step(f\"Error fetching SpaceX data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def create_launches_schema():\n",
    "    \"\"\"Define schema for SpaceX launches data\"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"flight_number\", IntegerType(), True),\n",
    "        StructField(\"mission_name\", StringType(), True),\n",
    "        StructField(\"launch_year\", StringType(), True),\n",
    "        StructField(\"launch_date_utc\", StringType(), True),\n",
    "        StructField(\"launch_success\", BooleanType(), True),\n",
    "        StructField(\"rocket\", MapType(StringType(), StringType()), True),\n",
    "        StructField(\"links\", MapType(StringType(), StringType()), True),\n",
    "        StructField(\"details\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "@measure_time\n",
    "def filter_launches(launches, year=None, success_status=None):\n",
    "    \"\"\"Filter launches by year and success status\"\"\"\n",
    "    log_step(f\"Filtering {len(launches)} launches (year: {year}, success: {success_status})\")\n",
    "    \n",
    "    try:\n",
    "        # Create DataFrame with explicit schema\n",
    "        schema = create_launches_schema()\n",
    "        rdd = spark.sparkContext.parallelize(launches)\n",
    "        df = spark.read.schema(schema).json(rdd.map(lambda x: json.dumps(x)))\n",
    "        \n",
    "        # Apply filters if provided\n",
    "        if year is not None:\n",
    "            df = df.filter(col(\"launch_year\") == str(year))\n",
    "        if success_status is not None:\n",
    "            df = df.filter(col(\"launch_success\") == success_status)\n",
    "            \n",
    "        filtered_data = [row.asDict() for row in df.collect()]\n",
    "        log_step(f\"Found {len(filtered_data)} matching launches after filtering\")\n",
    "        return filtered_data\n",
    "    except Exception as e:\n",
    "        log_step(f\"Error filtering launches: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "@measure_time\n",
    "def send_to_endpoint(data, endpoint_url):\n",
    "    \"\"\"Send filtered data to specified endpoint\"\"\"\n",
    "    log_step(f\"Sending {len(data)} records to {endpoint_url}\")\n",
    "    \n",
    "    try:\n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        response = requests.post(endpoint_url, data=json.dumps(data), headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        log_step(f\"Successfully sent data. Response status: {response.status_code}\")\n",
    "        log_step(f\"Response content: {response.text[:200]}...\")\n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        log_step(f\"Error sending data to endpoint: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main pipeline execution\"\"\"\n",
    "    try:\n",
    "        # Configuration parameters\n",
    "        launch_year = 2020  # Set to None to disable year filter\n",
    "        success_status = True  # Set to None to disable success filter\n",
    "        endpoint_url = \"https://httpbin.org/post\"\n",
    "        \n",
    "        log_step(\"Starting SpaceX Data Pipeline\")\n",
    "        \n",
    "        # Step 1: Fetch data\n",
    "        launches = fetch_spacex_launches()\n",
    "        \n",
    "        # Step 2: Filter data\n",
    "        filtered_launches = filter_launches(launches, year=launch_year, success_status=success_status)\n",
    "        \n",
    "        # Step 3: Send data\n",
    "        send_response = send_to_endpoint(filtered_launches, endpoint_url)\n",
    "        \n",
    "        log_step(\"Pipeline completed successfully!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_step(f\"Pipeline failed with error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Execute the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    pipeline_success = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce2c0bc3-ccb7-4ef0-87f4-5d184944d2dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2 iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b8337fc-117a-4447-9d5f-5480e5cec4d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType, IntegerType, MapType\n",
    "\n",
    "def log_step(message):\n",
    "    \"\"\"Helper function to log pipeline steps with timestamps\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"[{timestamp}] {message}\")\n",
    "\n",
    "def measure_time(func):\n",
    "    \"\"\"Decorator to measure execution time of functions\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        log_step(f\"Starting: {func.__name__}\")\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        log_step(f\"Completed: {func.__name__} in {end_time - start_time:.2f} seconds\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@measure_time\n",
    "def fetch_spacex_launches():\n",
    "    \"\"\"Fetch SpaceX launches data from API\"\"\"\n",
    "    url = \"https://api.spacexdata.com/v3/launches\"\n",
    "    log_step(f\"Fetching data from {url}\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        log_step(f\"Successfully fetched {len(data)} launches\")\n",
    "        return data\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        log_step(f\"Error fetching SpaceX data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def create_launches_schema():\n",
    "    \"\"\"Define schema for SpaceX launches data\"\"\"\n",
    "    return StructType([\n",
    "        StructField(\"flight_number\", IntegerType(), True),\n",
    "        StructField(\"mission_name\", StringType(), True),\n",
    "        StructField(\"launch_year\", StringType(), True),\n",
    "        StructField(\"launch_date_utc\", StringType(), True),\n",
    "        StructField(\"launch_success\", BooleanType(), True),\n",
    "        StructField(\"rocket\", MapType(StringType(), StringType()), True),\n",
    "        StructField(\"links\", MapType(StringType(), StringType()), True),\n",
    "        StructField(\"details\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "@measure_time\n",
    "def filter_launches(launches, year=None, success_status=None):\n",
    "    \"\"\"Filter launches by year and success status\"\"\"\n",
    "    log_step(f\"Filtering {len(launches)} launches (year: {year}, success: {success_status})\")\n",
    "    \n",
    "    try:\n",
    "        # Create DataFrame without using sparkContext\n",
    "        schema = create_launches_schema()\n",
    "        df = spark.createDataFrame(launches, schema=schema)\n",
    "        \n",
    "        # Apply filters if provided\n",
    "        if year is not None:\n",
    "            df = df.filter(col(\"launch_year\") == str(year))\n",
    "        if success_status is not None:\n",
    "            df = df.filter(col(\"launch_success\") == success_status)\n",
    "            \n",
    "        filtered_data = [row.asDict() for row in df.collect()]\n",
    "        log_step(f\"Found {len(filtered_data)} matching launches after filtering\")\n",
    "        return filtered_data\n",
    "    except Exception as e:\n",
    "        log_step(f\"Error filtering launches: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "@measure_time\n",
    "def send_to_endpoint(data, endpoint_url):\n",
    "    \"\"\"Send filtered data to specified endpoint\"\"\"\n",
    "    log_step(f\"Sending {len(data)} records to {endpoint_url}\")\n",
    "    \n",
    "    try:\n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        response = requests.post(endpoint_url, data=json.dumps(data), headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        log_step(f\"Successfully sent data. Response status: {response.status_code}\")\n",
    "        log_step(f\"Response content: {response.text[:200]}...\")\n",
    "        return response\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        log_step(f\"Error sending data to endpoint: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main pipeline execution\"\"\"\n",
    "    try:\n",
    "        # Configuration parameters\n",
    "        launch_year = 2020  # Set to None to disable year filter\n",
    "        success_status = True# Set to None to disable success filter\n",
    "        endpoint_url = \"https://httpbin.org/post\"\n",
    "        \n",
    "        log_step(\"Starting SpaceX Data Pipeline\")\n",
    "        \n",
    "        # Step 1: Fetch data\n",
    "        launches = fetch_spacex_launches()\n",
    "        \n",
    "        # Step 2: Filter data\n",
    "        filtered_launches = filter_launches(launches, year=launch_year, success_status=success_status)\n",
    "        \n",
    "        # Step 3: Send data\n",
    "        send_response = send_to_endpoint(filtered_launches, endpoint_url)\n",
    "        \n",
    "        log_step(\"Pipeline completed successfully!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_step(f\"Pipeline failed with error: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Execute the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    pipeline_success = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d41c4f87-9f46-474a-8888-d2e53ed0c9ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Filter DeepSeek-V3",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
